<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
	<channel>
		<title>Petar Marinov</title>
		<link>http://pmarinov.xyz/</link>
		<description>/dev/shared</description>
		<copyright>2020</copyright>
		<pubDate>Sun, 22 Mar 2020 21:36:33 GMT</pubDate>
		<lastBuildDate>Sun, 22 Mar 2020 21:36:33 GMT</lastBuildDate>
		<item>
			<title>Breaking change in Entity Framework Core provider for PostgreSQL 3.0</title>
			<link>http://pmarinov.xyz/posts/2020-03-22-breaking-change-in-entity-framework-core-provider-for-postgresql-3</link>
			<description>&lt;p&gt;Beware when upgrading form &lt;code&gt;Npgsql.EntityFrameworkCore.PostgreSQL 2.2&lt;/code&gt; and running &lt;code&gt;PostgreSQL 9.6&lt;/code&gt; or earlier.&lt;/p&gt;</description>
			<guid>http://pmarinov.xyz/posts/2020-03-22-breaking-change-in-entity-framework-core-provider-for-postgresql-3</guid>
			<pubDate>Sun, 22 Mar 2020 00:00:00 GMT</pubDate>
			<content:encoded>&lt;!-- # Breaking change in Entity Framework Core provider for PostgreSQL 3.0 --&gt;
&lt;p&gt;Beware when upgrading form &lt;code&gt;Npgsql.EntityFrameworkCore.PostgreSQL 2.2&lt;/code&gt; and running &lt;code&gt;PostgreSQL 9.6&lt;/code&gt; or earlier.&lt;/p&gt;
&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;Recently I wanted to add &lt;a href="https://docs.microsoft.com/en-us/aspnet/core/security/authentication/identity?view=aspnetcore-3.1&amp;amp;tabs=visual-studio"&gt;ASP.NET Core Identity&lt;/a&gt; to an application using PostgreSQL as a data storage. Since I prefer to manage database schema and migrations separately from the application (currently using &lt;a href="https://dbup.readthedocs.io/en/latest/"&gt;DbUp&lt;/a&gt;) I decided to use Entity Framework Core migrations to generate the SQL script for it.&lt;/p&gt;
&lt;h2 id="generating-the-migration-sql-script"&gt;Generating the migration SQL script&lt;/h2&gt;
&lt;p&gt;Using the Entity Framework Core tools 3.x is straight-froward.
(Given that you have added the necessary dependencies and defined your application's &lt;code&gt;IdentityDbContext&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;First make sure it is installed:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;dotnet tool install --global dotnet-ef
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Initialize the migrations in the project where your DbContext is:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;dotnet ef migrations add init
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then generate the SQL script for the DbContext (called ApplicationDbContext in my case):&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;dotnet ef migrations script -c ApplicationDbContext -o migration_script.sql
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Removed the SQL related to &lt;code&gt;__EFMigrationsHistory&lt;/code&gt; and my migration script was ready.
Then removed the migrations folder from my project and was ready to go.&lt;/p&gt;
&lt;p&gt;Ran the SQL script and... it failed!&lt;/p&gt;
&lt;h2 id="the-issue"&gt;The issue&lt;/h2&gt;
&lt;p&gt;I was quite surprized to see that the error was in the SQL:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ERROR: syntax error at or near &amp;quot;GENERATED&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the offending statement was:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;CREATE TABLE &amp;quot;AspNetRoleClaims&amp;quot; (
    &amp;quot;Id&amp;quot; integer NOT NULL GENERATED BY DEFAULT AS IDENTITY,
...    
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which on further research made perfect sense because this is a new syntax  &lt;code&gt;GENERATED AS IDENTITY&lt;/code&gt; for creating auto-incremented column available in PostgreSQL 10 and I was running version 9.6.&lt;/p&gt;
&lt;p&gt;I was using &lt;code&gt;Npgsql.EntityFrameworkCore.PostgreSQL 3.1.1&lt;/code&gt; and going through it I found a documented breaking change in &lt;code&gt;3.0.0&lt;/code&gt;:&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;em&gt;The default value generation strategy has changed from the older SERIAL columns to the newer IDENTITY columns, introduced in PostgreSQL 10.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can look for details here: &lt;a href="https://www.npgsql.org/efcore/release-notes/3.0.html"&gt;3.0 Release Notes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fortunately the fix was easy - just had to specify the PostgreSQL version I was targeting:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class ApplicationDbContext : IdentityDbContext
{
    public ApplicationDbContext(DbContextOptions options) : base(options)
    {
    }

    protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder)
    {
        optionsBuilder.UseNpgsql(&amp;quot;DefaultConnection&amp;quot;, o =&amp;gt; o.SetPostgresVersion(9, 6));
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After re-generating the migrations the older &lt;code&gt;SERIAL&lt;/code&gt; statement was used:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;CREATE TABLE &amp;quot;AspNetRoleClaims&amp;quot; (
    &amp;quot;Id&amp;quot; serial NOT NULL,
...    
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and everything went fine.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;PostgreSQL 10 introduced &lt;code&gt;GENERATED AS IDENTITY&lt;/code&gt; syntax aiming to replace &lt;code&gt;SERIAL&lt;/code&gt; for automatic assignment of unique value to a column. &lt;code&gt;Npgsql.EntityFrameworkCore.PostgreSQL 3.0.0&lt;/code&gt; takes advantage of it and uses it as a default when generating SQL migrations. Upgrading dependencies caught me off guard this time but the breaking change was well documented and my problem easily fixed.&lt;/p&gt;
</content:encoded>
		</item>
		<item>
			<title>Deleting large amounts of data from MS SQL Server database</title>
			<link>http://pmarinov.xyz/posts/2020-02-27-deleting-large-amounts-of-data-from-ms-sql-server-database</link>
			<description>&lt;p&gt;Deleting large amount of rows (like millions of them) from a table has a downside of being slow and making a transaction log file to explode in terms of size. Here is an approach that worked for me overcome these obstacles.&lt;/p&gt;</description>
			<guid>http://pmarinov.xyz/posts/2020-02-27-deleting-large-amounts-of-data-from-ms-sql-server-database</guid>
			<pubDate>Thu, 27 Feb 2020 00:00:00 GMT</pubDate>
			<content:encoded>&lt;!--- # Deleting large amounts of data from MS SQL Server database ---&gt;
&lt;p&gt;Deleting large amount of rows (like millions of them) from a table has a downside of being slow and making a transaction log file to explode in terms of size. Here is an approach that worked for me overcome these obstacles.&lt;/p&gt;
&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;My problem was that in a multi-tenant database a tenant has grown too much - to the point that it had to be moved to it's own database. The plan was to restore a backup of the database and delete all other tenant's data. The database had around 150 tables - for most of them deletions were not a problem - but for around 20 of them more than 10 million rows were to be deleted and for particular 5 tables more than 600 million.&lt;/p&gt;
&lt;h2 id="first-iteration-standard-deletions"&gt;First iteration: Standard deletions&lt;/h2&gt;
&lt;p&gt;For the first version of the deletion I used standard delete statements similar to:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;DELETE x FROM a_table x JOIN &amp;#64;TenantsToDelete t ON t.Id=x.TenantId; 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It worked fine on a small data set and was useful to confirm the deletion order and flush out some bad data and design oddities. Also it has no problem for few thousand to even tens of thousands of rows so it worked fine for the majority of the cases.&lt;br /&gt;
Where millions of rows were to be deleted it was taking long time and transaction log was growing rapidly. The full deletion script actually never ran to completion on production-grade data.&lt;/p&gt;
&lt;h2 id="second-iteration-batched-deletions"&gt;Second iteration: Batched deletions&lt;/h2&gt;
&lt;p&gt;For the large tables I tried batched deletes similar to:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;DECLARE &amp;#64;batch_size INT=5000;
delete_top:
  DELETE TOP(&amp;#64;batch_size) x FROM a_table x JOIN &amp;#64;TenantsToDelete t ON t.Id=x.TenantId;
  IF &amp;#64;batch_size = &amp;#64;&amp;#64;ROWCOUNT GOTO delete_top;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This version of the script was able to complete deletion of a single small-sized tenant for around 30 minutes, for a large one it took more than 15 hours - and I needed to remove around 40 of them. It was still no good.&lt;/p&gt;
&lt;h2 id="third-iteration-replace-delete-with-insert"&gt;Third iteration: Replace DELETE with INSERT&lt;/h2&gt;
&lt;p&gt;So &lt;code&gt;DELETE&lt;/code&gt; is slow and there is no much room for improvement. There is no &lt;em&gt;&amp;quot;bulk delete&amp;quot;&lt;/em&gt; kind of operation in MS SQL Server to boost the performance and I was not in a position to use &lt;code&gt;TRUNCATE&lt;/code&gt; or &lt;code&gt;DROP TABLE&lt;/code&gt; since I needed to preserve part of the data.&lt;/p&gt;
&lt;p&gt;While searching for solution I found this article about optimizing loading of data and the impact of minimally logged operations on I/O: &lt;a href="https://docs.microsoft.com/en-us/previous-versions/sql/sql-server-2008/dd425070(v=sql.100)?redirectedfrom=MSDN"&gt;The Data Loading Performance Guide&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In essence we can insert data fast (with minimal logging) when these conditions are met:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;database is in &lt;code&gt;SIMPLE&lt;/code&gt; or &lt;code&gt;BULK_LOGGED&lt;/code&gt; recovery mode,&lt;/li&gt;
&lt;li&gt;target table is a &lt;em&gt;heap table&lt;/em&gt; (without clustered index),&lt;/li&gt;
&lt;li&gt;a &lt;code&gt;WITH (TABLOCK)&lt;/code&gt; hint is used with the insert (allows exclusive lock on the target table).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is additional performance boost using the latter in SQL Server 2016 and higher - &lt;code&gt;INSERT...SELECT WITH (TABLOCK)&lt;/code&gt; may use parallel inserts.&lt;/p&gt;
&lt;p&gt;So back to my deletion problem - what if instead of delete I do the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a heap table with exactly the same structure as the one I want to delete from&lt;/li&gt;
&lt;li&gt;Move to the heap table data I want to &lt;strong&gt;keep&lt;/strong&gt; using &lt;code&gt;INSERT...SELECT WITH (TABLOCK)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Drop the original table (metadata-only operation, fast)&lt;/li&gt;
&lt;li&gt;Rename the heap table to original one's name&lt;/li&gt;
&lt;li&gt;Create indexes, constraints, references etc.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;TL;DR;&lt;/strong&gt;&lt;/em&gt; Yes, it worked and it was way faster without growing the transaction log since all operations were minimally logged.&lt;br /&gt;
Replacing deletion of my around 30 problematic tables with this technique lead to deletion of everything but a mid-sized tenant data to complete in 15 minutes.&lt;br /&gt;
Deletion of everything but the largest tenant (moving the most data) completed in 1 hour.&lt;/p&gt;
&lt;h3 id="generating-the-sql"&gt;Generating the SQL&lt;/h3&gt;
&lt;p&gt;Although I was quite happy with the performance, there was another problem - scripting the heap tables, moving the data and re-creating the indexes by hand is tedious and error prone (imagine doing it for 30 tables).&lt;/p&gt;
&lt;p&gt;SQL Server Management Studio has a lot of scripting capabilities and fortunately they are available via the &lt;a href="https://docs.microsoft.com/en-us/sql/relational-databases/server-management-objects-smo/sql-server-management-objects-smo-programming-guide?view=sql-server-ver15"&gt;SQL Management Objects&lt;/a&gt; (SMO). It is a set of .NET Framework assemblies meaning that they can be used from PowerShell also.&lt;/p&gt;
&lt;p&gt;Loading the assemblies and creating &lt;code&gt;Microsoft.SqlServer.Management.Smo.Server&lt;/code&gt; instance is the first step:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;[System.Reflection.Assembly]::LoadWithPartialName(&amp;quot;Microsoft.SqlServer.Smo&amp;quot;)
[System.Reflection.Assembly]::LoadWithPartialName(&amp;quot;Microsoft.SqlServer.ConnectionInfo&amp;quot;)

$connection = New-Object System.Data.SqlClient.SqlConnection $sqlConnectionString
$serverConnection = New-Object Microsoft.SqlServer.Management.Common.ServerConnection $connection

[Microsoft.SqlServer.Management.Smo.Server] $server = New-Object Microsoft.SqlServer.Management.Smo.Server $serverConnection
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To create a heap table:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;$scripter = New-Object Microsoft.SqlServer.Management.Smo.Scripter $server

$script = $scripter.Script(($server.Databases[$connection.Database].Tables[$table]))

# scripter can only generate scripts for existing objects so we need to change the table name 
$script[2] = $script[2].Replace(&amp;quot;CREATE TABLE [dbo].[$table]&amp;quot;, &amp;quot;CREATE TABLE [dbo].[$heapTable]&amp;quot;)
$script
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For moving the data, we can generate the columns list to use in the insert statement:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;$columnNames = $server.Databases[$connection.Database].Tables[$table].Columns | Select-Object $_ | ForEach-Object {$_.Name}
$columns = [System.String]::Join(', ', $columnNames)

$sql = New-Object System.Collections.Specialized.StringCollection
$sql.Add(&amp;quot;INSERT INTO $heapTable WITH(TABLOCK)&amp;quot;) &amp;gt; $null
$sql.Add(&amp;quot;($columns)&amp;quot;) &amp;gt; $null
$sql.Add(&amp;quot;SELECT&amp;quot;) &amp;gt; $null
$sql.Add(&amp;quot;$columns FROM $table &amp;quot;) &amp;gt; $null
$sql.Add(&amp;quot;$whereClause&amp;quot;) &amp;gt; $null

$sql
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are some pitfalls to consider:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;beware of &lt;code&gt;TIMESTAMP&lt;/code&gt; columns: you cannot insert them&lt;/li&gt;
&lt;li&gt;beware of &lt;code&gt;IDENTITY&lt;/code&gt; columns: I found it hard to change column to identity so create it as an identity and use &lt;code&gt;SET IDENTITY_INSERT ON/OFF&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;again for &lt;code&gt;IDENTITY&lt;/code&gt; columns: the scripter will capture the current identity seed when generating the script but it will be executed at some later point and the seed will be different. Thus in my script I capture the seed from the original table column after the data is moved and reseed the column of the heap table. To find the identity column you can use something like:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;$server.Databases[$connection.Database].Tables[$table].Columns | Where-Object { $_.Identity -eq $true } | Select-Object -First 1
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;depending on your &lt;code&gt;$whereClause&lt;/code&gt; you may need to prefix the columns in the select list, e.g. when using something like &lt;code&gt;SELECT x.ID FROM x_Table x JOIN y_Table y on y.Id=x.Id...&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before dropping the original table we need to remove all references to it's Primary Key:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;$inboundForeignKeys = $server.Databases[$connection.Database].Tables[$table].EnumForeignKeys()
foreach($foreignKey in $inboundForeignKeys)
{
    Write-Output &amp;quot;ALTER TABLE [dbo].[$($foreignKey['Table_Name'])] DROP CONSTRAINT [$($foreignKey['Name'])];&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you have triggers or schema-bound objects or anything preventing table to be drop should be taken care of - luckily in my case there was not.
Then we can drop the original table and rename the heap table. Nothing special required - &lt;code&gt;DROP TABLE&lt;/code&gt; and &lt;code&gt;sp_rename&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The last step is to create primary and foreign keys, defaults, constraints and indexes:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;# indexes, including primary key
$scripter = New-Object Microsoft.SqlServer.Management.Smo.Scripter $server
$scripter.Options.NoCollation = $true
$scripter.Options.DriPrimaryKey = $true
$scripter.Options.DriUniqueKeys = $true
$scripter.Options.ClusteredIndexes = $true
$scripter.Options.NonClusteredIndexes = $true
$scripter.Options.Indexes = $true
$scripter.Options.DriAll = $true

[Microsoft.SqlServer.Management.Smo.SqlSmoObject[]]$indexes = $server.Databases$connection.Database].Tables[$table].Indexes

$scripter.Script($indexes)

# default constraints
$columns = $server.Databases[$connection.Database].Tables[$table].Columns
foreach($column in $columns)
{
    if($column.DefaultConstraint)
    {
        $column.DefaultConstraint.Script()
    }
}

# foreign keys
[Microsoft.SqlServer.Management.Smo.SqlSmoObject[]]$foreignKeys = $server.Databases[$connection.Database].Tables[$table].ForeignKeys

$scripter.Script($foreignKeys)

# foreign keys referencing the table
$inboundForeignKeys = $server.Databases[$connection.Database].Tables[$table].EnumForeignKeys()
foreach($foreignKey in $inboundForeignKeys)
{
    [Microsoft.SqlServer.Management.Smo.SqlSmoObject[]]$key = $server.Databases[$connection.Database].Tables[$foreignKey['Table_Name']].ForeignKeys[$foreignKey['Name']]

    $scripter.Script($key)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are the most important the pieces I needed to generate the SQL.&lt;/p&gt;
&lt;p&gt;In practice I turned the initial deletion SQL script into a kind of template containing the standard deletion statements and occasionally a palceholder with the table name and where clause defining the data to be kept. Then a PowerShell script will read the template and replace the placeholders with SQL statements generated using the steps described above.&lt;/p&gt;
&lt;p&gt;Having automated SQL script generation based on the actual metadata has two main advantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;removes the repetitive (and boring) manual work&lt;/li&gt;
&lt;li&gt;when the schema changes (e.g. a table is added or altered, column is changed or dropped, index is created, etc.) the SQL script can be easily generated again&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Replacing a regular SQL deletes with inserting in heap table and dropping the original one has achieved a satisfying performance boost. It is a combination of SQL Server's data loading capabilities and minimally logged operations. In my case it worked good since I needed to keep (move) 20-30% of the table's data.&lt;/p&gt;
&lt;p&gt;Implementation is more complex than regular deletes so an automated SQL script generation comes handy.&lt;/p&gt;
&lt;p&gt;The downside of the solution is it's complexity so I would recommend it only as a last resort.&lt;/p&gt;
</content:encoded>
		</item>
	</channel>
</rss>