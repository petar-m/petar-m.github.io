<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
	<id>https://pmarinov.xyz/</id>
	<title>Petar Marinov</title>
	<link rel="self" href="https://pmarinov.xyz/" />
	<rights>2021</rights>
	<updated>2021-04-11T19:35:47Z</updated>
	<subtitle>/dev/shared</subtitle>
	<entry>
		<id>https://pmarinov.xyz/posts/2021-04-11-stateless-web-servers</id>
		<title>Stateless Web Servers</title>
		<link href="https://pmarinov.xyz/posts/2021-04-11-stateless-web-servers" />
		<updated>2021-04-11T00:00:00Z</updated>
		<content>&lt;p&gt;It all began when we discovered a strange behavior in one of our applications - users had been randomly logged out of the system.&lt;/p&gt;
&lt;h2 id="the-problem"&gt;The Problem&lt;/h2&gt;
&lt;p&gt;The system in question has multiple instances of web servers serving the UI and a load balancer with sticky connections configuration. We were not able to reproduce the issue in any of our environments and not all customers were affected. After a thorough investigation, it appeared that this was specific to some ISPs changing IPs and breaking the &amp;quot;stickiness&amp;quot; of connections.&lt;/p&gt;
&lt;h2 id="load-balancers-sticky-connections"&gt;Load Balancers Sticky Connections&lt;/h2&gt;
&lt;p&gt;Why were sticky connections selected as a load balancing strategy, to begin with? Probably because it had been a fast solution to a problem at the time being. With the rapid growth of the customer base and team racing to deliver features, no one was willing to give it a second thought. Sticky connections offer the promise that users would hit the same web server they connected the first time, so from a developer's point of view, it meant simplicity - the same as having only one web server.&lt;/p&gt;
&lt;h2 id="web-servers-state"&gt;Web Servers State&lt;/h2&gt;
&lt;p&gt;Why was connecting to the same web server important? Because there was an application state, namely the user session, stored in the web server's memory. &lt;a href="https://docs.microsoft.com/en-us/aspnet/signalr/overview/getting-started/introduction-to-signalr"&gt;SignalR&lt;/a&gt; also used the session and the transport protocol was fixed as long polling.&lt;/p&gt;
&lt;h2 id="choosing-a-solution-stateless-web-servers"&gt;Choosing a Solution - Stateless Web Servers&lt;/h2&gt;
&lt;p&gt;The problem could be solved by an even more sophisticated load balancer configuration. We felt, however, that increasing the infrastructure complexity will harm scalability in the long run. We decided to go with stateless web servers instead. Then a simpler and more efficient load balancing strategy, like least connections or round-robin, can be implemented. Now there is a catch - stateless web server does not mean that application does not have a state, it means that the state will not be bound to the web server's memory thus enabling any webserver to serve a request.&lt;/p&gt;
&lt;h2 id="achieving-statelessness"&gt;Achieving &amp;quot;Statelessness&amp;quot;&lt;/h2&gt;
&lt;p&gt;We needed to solve two application-level problems - moving the session state out of memory and ensuring SignalR was still working. The first step was to decide where the session state would be persisted. We researched three options - &lt;a href="https://docs.microsoft.com/en-us/previous-versions/aspnet/ms178586(v=vs.100)#state-server-mode"&gt;Microsoft Session State Server&lt;/a&gt;, &lt;a href="https://docs.microsoft.com/en-us/previous-versions/aspnet/ms178586(v=vs.100)#sql-server-mode"&gt;Microsoft SQL Server&lt;/a&gt;, and &lt;a href="https://docs.microsoft.com/en-us/azure/azure-cache-for-redis/cache-aspnet-session-state-provider"&gt;Redis&lt;/a&gt;. All of the options were easy to implement requiring a NuGet package dependency and configuration. Of course, they also required their specific service to run on a separate server, accessible from the web servers. Session State Server was dismissed as we were not sure how much support it will get in the future. MS SQL Server seemed like a too heavyweight solution, even using memory-optimized tables. We chose Redis because it gave us the best performance and we had more use for it in mind. Scaling SignalR boiled down to configure it to use &lt;a href="https://docs.microsoft.com/en-us/aspnet/signalr/overview/performance/scaleout-with-redis"&gt;Redis Backplane&lt;/a&gt;.&lt;br /&gt;
It is worth noting that any other resources bound to a server should be considered and moved to a shared location (for example - files on the local files system).&lt;/p&gt;
&lt;h2 id="implementation"&gt;Implementation&lt;/h2&gt;
&lt;h3 id="prerequisites"&gt;Prerequisites&lt;/h3&gt;
&lt;p&gt;During the proof of concept phase, we had a glimpse of the &lt;a href="https://nickcraver.com/blog/2020/02/11/binding-redirects/"&gt;&amp;quot;binding redirect hell&amp;quot;&lt;/a&gt; that was awaiting us. Targeting a bit older version of .NET Framework and having a ton of dependencies was not the ideal situation to be in, especially when you throw in the mix some transient dependencies on .NET Standard. After going through verbose MS Build logs countless times we decided that a move to .NET Framework 4.8 and SignalR 2 and updating and cleaning up dependencies will be best in the long term. The decision was reinforced by the fact that 4.8 is the last stop for the &amp;quot;full&amp;quot; framework and since we were not able to move to .NET Core, at least we could be on the latest and long-term supported version.&lt;/p&gt;
&lt;h3 id="session-state"&gt;Session State&lt;/h3&gt;
&lt;p&gt;Once upgrades were completed we moved to get the session state out of the server memory. We opted for the built-in binary serialization, meaning we needed to add Serializable attributes to a bunch of classes. Since the session state was used to put in whatever you want, it was abused to be somewhat of a cache for various stuff. We were surprised to discover that when serialized in some cases it can be up to 3MB! Imagine this serialized and moved over the network on every request. It took some work to trim it down to a more manageable size - around 100KB in worst cases. This started another long-term project which goal was to have a session state containing only a few strings and a bunch of ids, and a distributed cache to store frequently used data - but this is another story.&lt;br /&gt;
One important piece of configuration was to use the same encryption configuration and keys on all IIS instances. Since an auth cookie produced by one server will eventually end up sent to a different one, each web server should be able to understand it.&lt;/p&gt;
&lt;h3 id="signalr"&gt;SignalR&lt;/h3&gt;
&lt;p&gt;Scaling out SignalR with Redis was easy, again it took a NuGet package dependency and configuration. There were two caveats though - we had to explicitly set the protocol to WebSocket for all clients which in turn allowed skipping protocol negotiation. This, and the upgrade to SignalR 2, meant that the server session was not supported. It was not a big problem since the requests were still authenticated and we could set up a few parameters to be passed with each request. On the server, they were verified and then used to recreate the session object getting whatever was needed from a cache. We also replaced custom notifications filtering involving storing connections and users with SignalR groups.&lt;br /&gt;
We had windows services producing notifications that were sent to web servers via custom messaging, and in turn, web servers used SignalR to broadcast to clients. We were able to hook up our windows services to the SignalR directly leveraging SignalR backplane and groups. This greatly simplified the overall implementation.&lt;/p&gt;
&lt;h3 id="roll-out"&gt;Roll Out&lt;/h3&gt;
&lt;p&gt;Getting all this to production without disturbing customers could be tricky. We planned to do it in phases and have a few weeks between each phase deployment to root out and fix eventual problems. We also had a staging environment with a properly configured load balancer with the envisioned future production configuration.&lt;/p&gt;
&lt;p&gt;The first phase was the riskiest - migration to .NET Framework 4.8, SignalR 2, and WebSockets. There were some problems with WebSockets connections, generally related to customer's infrastructure. We found this &lt;a href="https://websocketstest.com"&gt;https://websocketstest.com&lt;/a&gt; to be quite handy when looking for connectivity issues, kudos to its creators and maintainers.&lt;/p&gt;
&lt;p&gt;The second phase was switching SignalR to use the Redis backplane. It was only a configuration so we had a fast rollback option. We had also implemented an Azure Service Bus backplane as an alternative if something went wrong. Both options were not used since the switch went without problems.&lt;/p&gt;
&lt;p&gt;Next was switching windows services to use the Redis backplane for broadcasting notifications, again without problems. This was a configuration too so a rollback to the custom solution was a matter of configuration.&lt;/p&gt;
&lt;p&gt;The third phase was moving the session state in Redis. Once more it was a matter of changing configuration. This went smoothly without any issues at all.&lt;/p&gt;
&lt;p&gt;All this done, there was the last and most important piece of the puzzle to complete the solution - changing the load balancer configuration. Because all of the required configurations were already in production and working as expected we were quite confident that will be without any problems - which was the case in the end.&lt;/p&gt;
&lt;p&gt;This completed the implementation of our solution and it solved the original problem, and as an additional benefit improved the scalability of our web servers.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;A move from stateful to stateless web servers is not easy, mostly because the software is often implemented with the assumption that all requests for a given session will be handled by the same server. With careful planning, implementation, and rollout, it is achievable with minimal disturbance of service. Stateless web servers are easier to scale and manage and applications are more resilient to crashes or restarts. In my case, this also surfaced some hidden problems and improved the system as a whole.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;It all began when we discovered a strange behavior in one of our applications - users had been randomly logged out of the system.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>https://pmarinov.xyz/posts/2020-09-16-a-case-of-decorators-with-microsoft-di</id>
		<title>A Case of Decorators with Microsoft DI</title>
		<link href="https://pmarinov.xyz/posts/2020-09-16-a-case-of-decorators-with-microsoft-di" />
		<updated>2020-09-16T00:00:00Z</updated>
		<content>&lt;p&gt;Microsoft.Extensions.DependencyInjection implementation does not support decoration or interception out of the box. Here is how I solved this for a specific case.&lt;/p&gt;
&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;In a project I worked on recently I was using &lt;a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.extensions.dependencyinjection?view=dotnet-plat-ext-3.1" target="_blank"&gt;Microsoft.Extensions.DependencyInjection container&lt;/a&gt;.
I used a variation of the &lt;a href="https://en.wikipedia.org/wiki/Command_pattern" target="_blank"&gt;Command Pattern&lt;/a&gt; to expose domain logic and I needed to guarantee that every command instance is created and executed in scope, and the scope is then disposed of.&lt;/p&gt;
&lt;p&gt;I wanted commands to be independent, allowing multiple commands to be executed in parallel, or resolved and executed within &lt;a href="https://docs.microsoft.com/en-us/aspnet/core/fundamentals/host/hosted-services?view=aspnetcore-3.1&amp;amp;tabs=visual-studio" target="_blank"&gt;hosted services&lt;/a&gt; or in fire-and-forget scenarios. To illustrate a particular problem: let's say I have two commands depending on EF DbContext, and something depending on both.
Now if DbContext is scoped then I will get two command instances depending internally on the same DbContext instance and if executed in parallel there will be a problem since &lt;a href="https://docs.microsoft.com/en-us/ef/core/miscellaneous/configuring-dbcontext#avoiding-dbcontext-threading-issues" target="_blank"&gt;DbContext is not thread-safe&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="images/a-case-of-decorators-with-microsoft-di/shared_dbcontext.svg" class="img-fluid" alt="shared DbContext" /&gt;&lt;/p&gt;
&lt;p&gt;I could register the DbContext as transient but then I have to be sure that scope is disposed of according to &lt;a href="https://docs.microsoft.com/en-us/aspnet/core/fundamentals/dependency-injection?view=aspnetcore-3.1#general-idisposable-guidelines" target="_blank"&gt;IDisposable guidelines&lt;/a&gt;. It is not a problem if my domain lives in ASP .NET Core application since requests are executed in scope by design.
For windows services, hosted services, or console applications though the scope should be managed and I did not want this to be a concern in my domain.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Decorator_pattern" target="_blank"&gt;Decorator&lt;/a&gt; design pattern seemed like a way to solve it.&lt;/p&gt;
&lt;h2 id="what-i-wanted"&gt;What I Wanted&lt;/h2&gt;
&lt;p&gt;I had a generic abstraction similar to:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public interface ICommand&amp;lt;TAction&amp;gt;
{
    Task ExecuteAsync(TAction action);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and a simplified implementation can be:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class PrintHelloWorld{ }

public class HelloWorld : ICommand&amp;lt;PrintHelloWorld&amp;gt;
{
    public async Task ExecuteAsync(PrintHelloWorld action) 
        =&amp;gt; Console.WriteLine(&amp;quot;Hello World!&amp;quot;);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I wanted my domain commands to implement &lt;code&gt;ICommand&amp;lt;TAction&amp;gt;&lt;/code&gt; and when used by a consumer, something behind the scene to create a scope, resolve the command, execute it, and dispose of the created scope.&lt;/p&gt;
&lt;p&gt;&lt;img src="images/a-case-of-decorators-with-microsoft-di/decorators.svg" class="img-fluid" alt="decorators" /&gt;&lt;/p&gt;
&lt;p&gt;I will describe how decoration can be achieved in this case. If you are interested in a more general approach you can jump down to the &lt;a href="#alternatives"&gt;Alternatives&lt;/a&gt; section.&lt;/p&gt;
&lt;h2 id="implementing-a-decorator-to-control-scope"&gt;Implementing a Decorator to Control scope&lt;/h2&gt;
&lt;p&gt;So my decorator for controlling scope was something like:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class ScopedCommand&amp;lt;TAction&amp;gt; : ICommand&amp;lt;TAction&amp;gt;
{
    private IServiceScopeFactory _scopeFactory;

    public ScopedCommand(IServiceScopeFactory scopeFactory)
        =&amp;gt; _scopeFactory = scopeFactory;

    public async Task ExecuteAsync(TAction action)
    {
        using var scope = _scopeFactory.CreateScope();
        var command = scope.ServiceProvider.GetRequiredService&amp;lt;ICommand&amp;lt;TAction&amp;gt;&amp;gt;();
        await command.ExecuteAsync(action);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nice and easy so far but when I did the registrations I realized it won't work.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;serviceCollection.AddTransient&amp;lt;ICommand&amp;lt;PrintHelloWorld&amp;gt;, HelloWorld&amp;gt;()
                 .AddSingleton&amp;lt;ICommand&amp;lt;PrintHelloWorld&amp;gt;, ScopedCommand&amp;lt;PrintHelloWorld&amp;gt;&amp;gt;();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since I had two registrations for &lt;code&gt;ICommand&amp;lt;PrintHelloWorld&amp;gt;&lt;/code&gt; the call to &lt;code&gt;GetRequiredService&amp;lt;ICommand&amp;lt;PrintHelloWorld&amp;gt;&amp;gt;()&lt;/code&gt; will always resolve to &lt;code&gt;ScopedCommand&amp;lt;PrintHelloWorld&amp;gt;&lt;/code&gt; (last registered wins), and I can't get the &lt;code&gt;PrintPrintHelloWorld&lt;/code&gt; instance. In fact, the &lt;code&gt;ExecuteAsync()&lt;/code&gt; call ended up in &lt;code&gt;StackOverflowException&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I could get both instances if I used &lt;code&gt;GetServices()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using(var scope = _scopeFactory.CreateScope())
{
    var commands = scope.ServiceProvider.GetServices&amp;lt;ICommand&amp;lt;TAction&amp;gt;&amp;gt;() as ICommand&amp;lt;TAction&amp;gt;[];
    await command[0].ExecuteAsync(action); 
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first registered is the actual command implementation and the second one is the decorator. This works... almost. What if a consumer of this command requests all  &lt;code&gt;ICommand&amp;lt;PrintHelloWorld&amp;gt;&lt;/code&gt; implementations?&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public HelloWorldPrinter(IEnumerable&amp;lt;ICommand&amp;lt;PrintHelloWorld&amp;gt;&amp;gt; commands)
{
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a valid request to the container and it will return both implementations.&lt;/p&gt;
&lt;p&gt;Yet another problem is what if I had two or more implementations of  &lt;code&gt;ICommand&amp;lt;PrintHelloWorld&amp;gt;&lt;/code&gt; (not taking into account decorators)?&lt;/p&gt;
&lt;p&gt;To solve the problem I registered the &lt;code&gt;ICommand&amp;lt;TAction&amp;gt;&lt;/code&gt; implementation as itself instead of its interface. But then how &lt;code&gt;ScopedCommand&amp;lt;TAction&amp;gt;&lt;/code&gt; will know which &lt;code&gt;ICommand&amp;lt;TAction&amp;gt;&lt;/code&gt; implementation to resolve?&lt;br /&gt;
Appears that this information can be passed as generic type argument:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class ScopedCommand&amp;lt;TAction, TImplementation&amp;gt; : ICommand&amp;lt;TAction&amp;gt; 
                                                       where TImplementation : ICommand&amp;lt;TAction&amp;gt;
{
    private IServiceScopeFactory _scopeFactory;

    public ScopedCommand(IServiceScopeFactory scopeFactory)
        =&amp;gt; _scopeFactory = scopeFactory;

    public async Task ExecuteAsync(TAction action)
    {
        using var scope = _scopeFactory.CreateScope();
        var command = scope.ServiceProvider.GetRequiredService&amp;lt;TImplementation&amp;gt;();
        await command.ExecuteAsync(action);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;so now the registration looked like:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;serviceCollection.AddTransient&amp;lt;HelloWorld&amp;gt;()
                 .AddSingleton&amp;lt;ICommand&amp;lt;PrintHelloWorld&amp;gt;, ScopedCommand&amp;lt;PrintHelloWorld, HelloWorld&amp;gt;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and it can be generalized as &lt;code&gt;ServiceCollection&lt;/code&gt; extension method:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public static IServiceCollection AddCommand&amp;lt;TAction, TImplementation&amp;gt;(this IServiceCollection serviceCollection) 
    where TImplementation : class, ICommand&amp;lt;TAction&amp;gt;
{
    return serviceCollection.AddTransient&amp;lt;TImplementation&amp;gt;()
                            .AddSingleton&amp;lt;ICommand&amp;lt;TAction&amp;gt;, ScopedCommand&amp;lt;TAction, TImplementation&amp;gt;&amp;gt;();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I had implementations of &lt;code&gt;ICommand&amp;lt;TAction&amp;gt;&lt;/code&gt; in my domain and using the &lt;code&gt;AddCommand&amp;lt;TAction, TImplementation&amp;gt;()&lt;/code&gt; extension guaranteed me that consumers will actually get &lt;code&gt;ScopedCommand&amp;lt;TAction, TImplementation&amp;gt;&lt;/code&gt; as implementations.&lt;/p&gt;
&lt;p&gt;This had somewhat of a lazy loading effect: when a consumer gets an implementation it will actually be the &lt;code&gt;ScopedCommand&amp;lt;TAction, TImplementation&amp;gt;&lt;/code&gt; singleton. Only when the  &lt;code&gt;ExecuteAsync(TAction action)&lt;/code&gt; method is called the actual implementation will be created, executed, and disposed of. Consequent calls to &lt;code&gt;ExecuteAsync(TAction action)&lt;/code&gt; will be executed on a new instance.&lt;/p&gt;
&lt;p&gt;As an example this imaginary implementation will be ok because there will be no implicit dependencies on scoped instances:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class Processor
{
    private IEnumerable&amp;lt;ICommand&amp;lt;ProcessSomeData&amp;gt;&amp;gt; _processors;

    public Processor(IEnumerable&amp;lt;ICommand&amp;lt;ProcessSomeData&amp;gt;&amp;gt; processors)
        =&amp;gt; _processors = processors;

    public async Task ProcessAsync()
    {
        var tasks = _processors.Select(x =&amp;gt; x.ExecuteAsync(new ProcessSomeData()))
                               .ToArray();
        await Task.WhenAll(tasks);
    }

    public void FireAndForget()
    {
        foreach(var processor in _processors)
        {
            _ = Task.Run(async () =&amp;gt; await processor.ExecuteAsync(new ProcessSomeData()));
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="adding-command-decorators"&gt;Adding Command Decorators&lt;/h2&gt;
&lt;p&gt;Having &lt;code&gt;ScopedCommand&amp;lt;TAction, TImplementation&amp;gt;&lt;/code&gt; in place created an opportunity to add any number of decorators between it and the actual &lt;code&gt;ICommand&amp;lt;TAction&amp;gt;&lt;/code&gt; that is decorated. I introduced a base class for command decorators using method injection (it will be used by &lt;code&gt;ScopedCommand&lt;/code&gt; to wire-up decorators):&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt; public abstract class CommandDecorator&amp;lt;TAction&amp;gt; : ICommand&amp;lt;TAction&amp;gt;
 {
    protected ICommand&amp;lt;TAction&amp;gt; Command { get; private set; }
    public abstract Task ExecuteAsync(TAction action);
    public void SetDecoratee(ICommand&amp;lt;TAction&amp;gt; command) =&amp;gt; Command = command;
 }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An example decorator can be:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class SampleHelloWorldDecorator : CommandDecorator&amp;lt;PrintHelloWorld&amp;gt;
{
    public override async Task ExecuteAsync(PrintHelloWorld action)
    {
        Console.WriteLine(&amp;quot;----------&amp;quot;);
        await Command.ExecuteAsync(action);
        Console.WriteLine(&amp;quot;----------&amp;quot;);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The registration is straight-forward. Once again the trick is not to register the decorator by &lt;code&gt;ICommand&amp;lt;TAction&amp;gt;&lt;/code&gt; interface. An extension method was useful to convey intent:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public static IServiceCollection DecorateCommand&amp;lt;TAction, TImplementation&amp;gt;(this IServiceCollection serviceCollection) 
    where TImplementation : CommandDecorator&amp;lt;TAction&amp;gt;
{
    return serviceCollection.AddTransient&amp;lt;CommandDecorator&amp;lt;TAction&amp;gt;, TImplementation&amp;gt;();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And finally &lt;code&gt;ScopedDecorator&amp;lt;TAction&amp;gt;&lt;/code&gt; implementation chained all decorators right before execution of the actual command:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class ScopedCommand&amp;lt;TAction, TImplementation&amp;gt; : ICommand&amp;lt;TAction&amp;gt; 
                                                       where TImplementation : ICommand&amp;lt;TAction&amp;gt;
{
    private IServiceScopeFactory _scopeFactory;

    public ScopedCommand(IServiceScopeFactory scopeFactory)
        =&amp;gt; _scopeFactory = scopeFactory;

    public async Task ExecuteAsync(TAction action)
    {
        using var scope = _scopeFactory.CreateScope();
        var command = scope.ServiceProvider.GetRequiredService&amp;lt;TImplementation&amp;gt;();
        
        var decorators = scope.ServiceProvider.GetServices&amp;lt;CommandDecorator&amp;lt;TAction&amp;gt;&amp;gt;() as CommandDecorator&amp;lt;TAction&amp;gt;[];
        if(decorators?.Length &amp;gt; 0)
        {
            decorators[0].SetDecoratee(command);
            for (int i = 1; i &amp;lt; decorators.Length ; i++)
            {
                decorators[i].SetDecoratee(decorators[i - 1]);
            }

            await decorators[^1].ExecuteAsync(action);
        }
        else
        {
            await command.ExecuteAsync(action);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The order of decorators is determined by the order of registration. Their execution is in reverse order - the last registered will be the first to execute and so on. Have to admit that registration and execution order always makes me stop and think about it.&lt;/p&gt;
&lt;p&gt;Having for example:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;serviceCollection.AddCommand&amp;lt;PrintHelloWorld, HelloWorld&amp;gt;()
                 .DecorateCommand&amp;lt;PrintHelloWorld, SampleHelloWorldDecorator&amp;gt;()
                 .DecorateCommand&amp;lt;PrintHelloWorld, AnotherDecorator&amp;gt;();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will have execution order: &lt;code&gt;AnotherDecorator -&amp;gt; SampleHelloWorldDecorator -&amp;gt; HelloWorld&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This satisfied my requirements. I added additional classes to support another interface I had: &lt;code&gt;IQuery&amp;lt;TAction, TResult&amp;gt;&lt;/code&gt;. The implementation is almost the same with one more generic type argument for the result.&lt;/p&gt;
&lt;p&gt;This solution is not perfect but worked for me, so I decided to use it without trying to further generalize it. If more sophisticated use cases come up in the future I have the choice of implementing them or scrape the whole thing in favor of something else.&lt;/p&gt;
&lt;p&gt;&lt;a name="alternatives"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="alternatives"&gt;Alternatives&lt;/h2&gt;
&lt;p&gt;This solution fits my requirements and is not a general-purpose one. There are some alternatives I found (by the time of writing):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first advice is to switch to a different container that supports decorators out of the box.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/khellang/Scrutor" target="_blank"&gt;Scrutor&lt;/a&gt; provides assembly scanning and decoration extensions for Microsoft.Extensions.DependencyInjection.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/VikKol/Microsoft.DI.Decorator" target="_blank"&gt;Microsoft.DI.Decorator&lt;/a&gt; provides a simple implementation of Decorators in Microsoft.Extensions.DependencyInjection.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/lawrence-laz/Decor.NET" target="_blank"&gt;Decor.NET&lt;/a&gt; provides a nice and simple way to execute any code before and after any other method.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Lack of support for decoration by Microsoft.Extensions.DependencyInjection implementation hampers one of the benefits of using a container. Nevertheless, it can be overcome either with a custom implementation suiting your needs or by using a third-party solution.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;Microsoft.Extensions.DependencyInjection implementation does not support decoration or interception out of the box. Here is how I solved this for a specific case.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>https://pmarinov.xyz/posts/2020-07-25-incremental-code-restructuring</id>
		<title>Incremental Code Restructuring</title>
		<link href="https://pmarinov.xyz/posts/2020-07-25-incremental-code-restructuring" />
		<updated>2020-07-25T00:00:00Z</updated>
		<content>&lt;p&gt;Feature Switch and Branch by Abstraction are techniques usually mentioned in the context of Trunk Based Development. Nevertheless, when used together, they become a powerful tool for changing existing code.&lt;/p&gt;
&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;I wager every successful software that has been growing rapidly for some years has one of those: a component that has too many responsibilities and accidentally coupling various pieces of the system depending on it. In my case, it was an important piece of code that was not designed to be unit-testable (therefore had no single unit test) and was quite complicated on top of it. To do an in-place refactoring we had to first create &amp;quot;seams&amp;quot; so it can be covered with unit tests - a risky affair. So instead of refactoring, we decided to extract functionalities that don't belong to it by re-implementing them.&lt;/p&gt;
&lt;h2 id="feature-switch"&gt;Feature Switch&lt;/h2&gt;
&lt;p&gt;The first step was to introduce a &lt;a href="https://en.wikipedia.org/wiki/Feature_toggle"&gt;Feature Switch&lt;/a&gt; functionality in our system. I was looking for the simplest thing that will do the job so I ended up with a configuration file values. This means that feature switch configuration is loaded up once at startup and can be changed only with a restart. The approach was good enough - to change a feature switch we had to change the value in our deployment system where we can manage values per environment and redeploy.&lt;/p&gt;
&lt;p&gt;That said there is no need to roll your feature switch implementation - there are plenty of options both open source and commercial.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: I do a distinction between feature switches and system settings. Although similar, they represent different concepts: feature switches are temporary and preferably short-lived constructs, whether system settings represent multiple different valid behaviors of the system.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="branch-by-abstraction"&gt;Branch by Abstraction&lt;/h2&gt;
&lt;p&gt;Having feature switches in place it was easy to apply &lt;a href="https://www.martinfowler.com/bliki/BranchByAbstraction.html"&gt;Branch by Abstraction&lt;/a&gt; technique.&lt;/p&gt;
&lt;p&gt;&lt;img src="images/incremental-code-restructuring/feature-switch.svg" class="img-fluid" alt="branch by abstraction" /&gt;&lt;/p&gt;
&lt;p&gt;Already having an interface we introduced second implementation and a &amp;quot;main&amp;quot; feature switch - based on it we configured our IoC container.&lt;/p&gt;
&lt;h2 id="introduce-new-implementation"&gt;Introduce New Implementation&lt;/h2&gt;
&lt;p&gt;The new implementation was nothing but a proxy to the original implementation, dispatching all calls to it. Next, we singled out a functionality we wanted to extract and introduced a specific feature switch to control dispatching the calls to the original or new implementation.&lt;/p&gt;
&lt;p&gt;&lt;img src="images/incremental-code-restructuring/new-implementation.svg" class="img-fluid" alt="branch by abstraction" /&gt;&lt;/p&gt;
&lt;p&gt;This turned the proxy into an adapter translating the original input and output to the new one. It allowed us to work on the new implementation in isolation without changing the original. The client code that was using the original interface was oblivious that the implementation was different. The feature switch allowed us to gradually roll it through QA, staging, and finally live environments.&lt;/p&gt;
&lt;h2 id="transition-to-new-implementation-and-clean-up"&gt;Transition to New Implementation and Clean Up&lt;/h2&gt;
&lt;p&gt;The new implementation was fully covered by unit tests and necessary integration tests. After using it for some time in live environments without issues we were quite confident that it had the same behavior and does not introduce regressions. It was time for the final step.&lt;/p&gt;
&lt;p&gt;&lt;img src="images/incremental-code-restructuring/transition.svg" class="img-fluid" alt="transition to new implementation" /&gt;&lt;/p&gt;
&lt;p&gt;We removed the methods from the original interface and their corresponding implementation and replaced usages with the new implementation. Also removed the feature switch related to it. Note that this is a breaking change that brings some risk and should be executed with care. Since the new interface and implementation were focused on single responsibility the dependent components were few.&lt;/p&gt;
&lt;p&gt;The &amp;quot;main&amp;quot; feature switch remained in place for the next piece of work - we will repeat the same cycle for the remaining features that should be extracted from the original implementation.&lt;/p&gt;
&lt;h2 id="fun-fact"&gt;Fun Fact&lt;/h2&gt;
&lt;p&gt;This technique is well known but does not have a distinctive name. Sometimes I refer to it as &amp;quot;Non-Destructive Refactoring&amp;quot; which is true to some extent. For some reason, it sells better to management than &amp;quot;Branch By Abstraction&amp;quot; when you need to convince them it should be done. &amp;quot;Incremental code replacement&amp;quot; is more accurate, but the technique itself is quite simple, and all these names make it sound somewhat more complicated.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;By using Feature Switch and Branch by Abstraction techniques we achieved smooth transition and we were confident that we can switch implementations along the way only with a configuration change. We achieved the goal to have comprehensive unit and integration tests providing a safety net for future development and to remove a lot of accidental complexity accumulated for years.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;Feature Switch and Branch by Abstraction are techniques usually mentioned in the context of Trunk Based Development. Nevertheless, when used together, they become a powerful tool for changing existing code.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>https://pmarinov.xyz/posts/2020-05-25-layers-and-features</id>
		<title>Layers and Features</title>
		<link href="https://pmarinov.xyz/posts/2020-05-25-layers-and-features" />
		<updated>2020-05-25T00:00:00Z</updated>
		<content>&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;Layered Architecture is a well known and proven approach when developing software. But should it always be the default choice?&lt;/p&gt;
&lt;h2 id="layered-architecture-problems"&gt;Layered Architecture Problems&lt;/h2&gt;
&lt;p&gt;By &amp;quot;Layered Architecture&amp;quot; I refer to the class of software architectures using &lt;a href="https://en.wikipedia.org/wiki/Layer_(object-oriented_design)"&gt;Layers&lt;/a&gt; and the &lt;a href="https://en.wikipedia.org/wiki/Dependency_inversion_principle"&gt;Dependency Inversion Principle&lt;/a&gt;. &lt;a href="https://jeffreypalermo.com/2008/07/the-onion-architecture-part-1/"&gt;Onion&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Hexagonal_architecture_(software)"&gt;Hexagonal&lt;/a&gt;, Ports and Adapters, &lt;a href="https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html"&gt;Clean&lt;/a&gt; are some notable definitions.&lt;/p&gt;
&lt;p&gt;There is no doubt in the benefits of layers and Layered Architecture. Therefore I will go directly to what bothers me about it.&lt;/p&gt;
&lt;p&gt;&lt;img src="images/layers.svg" class="img-fluid" alt="layers" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;It is often used as a system-wide architecture. To implement the first features, the layers need to be defined upfront. This means that once layers are in place, every feature should comply with them. The problem is that a decision having such a big impact on future development is made too early.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Adding a feature requires changes in multiple layers. In the early stages of development or when prototyping some new feature, this can be a burden. Also, future maintenance and development can become complicated.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Layers enforce abstraction - good layer design is to have an internal and external representation of the entities it handles. There are cases when this does not bring any value, e.g. simple &lt;a href="https://en.wikipedia.org/wiki/Create,_read,_update_and_delete"&gt;CRUD&lt;/a&gt; or reporting.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sometimes having to go through all layers is an overhead. This is why some define layers as &amp;quot;closed&amp;quot; (it is mandatory to always go through it) and &amp;quot;open&amp;quot; (can be &amp;quot;skipped&amp;quot; in some cases). I don't find this a good practice. Once allowed such &amp;quot;shortcuts&amp;quot; will repeat over time leading to probably &amp;quot;open&amp;quot; all layers thus defeating the purpose.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Features tend to have specific requirements. Although layers promote reusability in most cases each layer will have abstractions serving specific features.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So what can be used as an alternative?&lt;/p&gt;
&lt;h2 id="vertical-slices"&gt;Vertical Slices&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://jimmybogard.com/vertical-slice-architecture/"&gt;Vertical Slices Architecture&lt;/a&gt; moves the boundaries from layers to features. The general idea is to group what is likely to change together.&lt;/p&gt;
&lt;p&gt;&lt;img src="images/verticalslices.svg" class="img-fluid" alt="layers" /&gt;&lt;/p&gt;
&lt;p&gt;Verticals slices do not exclude layers. The difference is that decisions can be made on a feature basis. Some features may benefit from a rich domain model and more strict layering and abstractions. For others, a simple &lt;a href="https://www.martinfowler.com/eaaCatalog/transactionScript.html"&gt;Transaction Script&lt;/a&gt; may be suitable, or &lt;a href="https://martinfowler.com/eaaDev/EventSourcing.html"&gt;Event Sourcing&lt;/a&gt; or &lt;a href="https://en.wikipedia.org/wiki/Command%E2%80%93query_separation#Command_query_responsibility_segregation"&gt;CQRS&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are some general considerations when using this approach:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Make features as independent as possible. Define a clear public interface for other features or the UI layer to use while keeping internal details private.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Be very careful when introducing shared components - do not introduce accidental coupling. That said there will be shared domain models and services that will emerge over time. It is normal to have a shared database at least in the beginning.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Strive toward reusable Application Infrastructure - security, logging, transactions, exception handling, lifetime scopes. All features should be able to benefit from it and concentrate on the business problem they solve.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Do not abandon the Dependency Inversion Principle - there are still benefits from decoupling within a feature.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Simple in-memory events or messaging can act as an additional way of decoupling features.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;My preference is to keep the UI in a separate layer (probably physically separated e.g. in different assembly/package/module). This gives me the flexibility to use different hosts for the application - web server, console application, or system service.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="are-we-done-with-the-architecture"&gt;Are We Done With the Architecture?&lt;/h2&gt;
&lt;p&gt;Unfortunately, the answer is no. Generally speaking, architecture is never done when the software is still under development. I see Vertical Slices as something to start with, not the final thing. It requires attention to the code while it evolves. It means continuous refactoring and restructuring while getting more knowledge for the problem being solved.&lt;/p&gt;
&lt;p&gt;These are some cases that can occur:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Two features can depend on each other with circular dependency. Probably this comes from wrongly defined feature boundary and they should be merged in one feature.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Multiple features depend on shared service. It is worth revisiting the design and review feature boundaries.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Code duplication in multiple features. Investigate whether it is really a code duplication or just a similar code in a different context. If it is really a duplication then it probably should be extracted.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Low cohesion within a feature. Probably the feature should be split into separate features.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Discover cohesion on the domain level. It possible to find out that the domain itself can be split into parts - a group of features may use given domain entities and a different group can use other entities with little to no crossing points.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As the requirements change over time there are multiple directions to go. It may be appropriate to start extracting features into &lt;a href="https://en.wikipedia.org/wiki/Microservices"&gt;Microservices&lt;/a&gt;. The feature's public interface can become an API call. Events feature produces or consumes can be put on an external message queue.&lt;/p&gt;
&lt;p&gt;If it makes sense there is nothing wrong to continue with a monolithic application. Then you can go on with &lt;a href="https://www.kamilgrzybek.com/design/modular-monolith-primer/"&gt;Modular Monolith&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Layered Architecture solves a certain class of problems perfectly. But not every project should use it from the start. There is no single architecture that will fit every use case. A project may evolve to layers but should be also flexible enough to be able to accommodate different approaches. Vertical Slices is one possible way to start.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;Layered Architecture is a well known and proven approach when developing software. But should it always be the default choice?&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>https://pmarinov.xyz/posts/2020-05-03-structuring-data-access</id>
		<title>Structuring Data Access</title>
		<link href="https://pmarinov.xyz/posts/2020-05-03-structuring-data-access" />
		<updated>2020-05-03T00:00:00Z</updated>
		<content>&lt;!-- # Structuring Data Access --&gt;
&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;Best practices in software development dictate that data storage and retrieval should be treated as an implementation detail. In practice it is easier said than done though - there are endless debates about how much abstraction is needed, whether the Repository Pattern is still relevant and if yes how to implement it, are ORMs good or evil, Micro ORMs and so on.&lt;/p&gt;
&lt;p&gt;Here are some solutions with their strengths and weaknesses.&lt;/p&gt;
&lt;h2 id="full-encapsulation"&gt;Full Encapsulation&lt;/h2&gt;
&lt;p&gt;&amp;quot;Full Encapsulation&amp;quot; is inspired by Uncle Bob's &lt;a href="https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html"&gt;Clean Architecture&lt;/a&gt; - have an abstraction with simple data structures going in and out. Internally the data access implementation will map these simple data structures to storage entities that suit best the libraries or frameworks used. In terms of total decoupling from any storage technology, this is the best approach. Its strict abstraction gives a lot of flexibility - i.e. a team can work on the data access details or multiple data access implementations can be switched or &lt;a href="https://en.wikipedia.org/wiki/Polyglot_persistence"&gt;Polyglot Persistence&lt;/a&gt; strategy can be utilized. Some implementations guidelines:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Organize abstractions and implementations to be in different assemblies (packages, modules) (&lt;a href="https://www.martinfowler.com/eaaCatalog/separatedInterface.html"&gt;Separated interface&lt;/a&gt;). I have often seen both in the same place - from a practical point of view this is not a bad thing but if you think about different implementation you will need to depend on all the concrete stuff along with the abstractions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Prefer persistence ignorant abstractions (i.e. do not expose a &lt;code&gt;Save()&lt;/code&gt; method) - each is an atomic operation by itself. It is possible to have multiple abstractions to act as an atomic action by introducing a variation of &lt;a href="https://www.martinfowler.com/eaaCatalog/unitOfWork.html"&gt;Unit Of Work&lt;/a&gt;. I will not go this way since it introduces a significant amount of complexity - individual abstractions will lose their autonomy and probably will have implicit dependencies, additional authority (the Unit of Work) should be managed and ultimately it will complicate usage from a consumer perspective.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Consider some form of &lt;a href="https://en.wikipedia.org/wiki/Specification_pattern"&gt;Specification&lt;/a&gt; for retrieving data. It does not have to be a strict implementation of the pattern, rather it's intent is to provide a way for declaring some aspect of the data we are interested in. This will simply allow for grouping similar data access patterns together i.e. if we have orders we can specify a time range, customer, product, etc. I would not go too far with this trying to define every possible case. Instead, when complexity grows beyond a certain point I would split in different abstractions i.e. customer's orders, product's orders, etc. - each having appropriate &amp;quot;specification&amp;quot;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There is no need for the abstraction to be about a single entity - it can represent a series of actions serving specific business case, i.e. storing related data in multiple database tables or pulling data from multiple sources to generate a report.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Prefer having a single public method. This will keep the focus on single functionality and make it simple to use.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sometimes having abstraction for input and output data may be overkill - than &lt;a href="https://en.wikipedia.org/wiki/Data_transfer_object"&gt;DTOs&lt;/a&gt; can be used instead. In both cases, I would not add any behavior to them.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Working on the data access in this setup is an absolute joy - being shielded from other parts of the system gives you freedom. You can use whatever libraries/frameworks, technologies, and data sources you need. This is the place where choice actually matters - you will choose a technology/framework/library because you'll put to use its unique features. At the same time, the &lt;a href="https://en.wikipedia.org/wiki/Leaky_abstraction"&gt;&amp;quot;leakiness&amp;quot;&lt;/a&gt; of the abstraction can be minimal. The testing strategy is also clear - integration tests for the implementation and mocks for unit tests.&lt;/p&gt;
&lt;p&gt;Unfortunately, such strict abstraction also has its downsides:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The number of abstractions, implementations, and mappings will grow in time since usually, each business case will become to have its unique requirements. Striking the balance between reusability and simplicity may become very tricky and hard to recognize.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A business case will become &amp;quot;stretched&amp;quot; through layers which may lead to having a hard time investigating issues or trying to get the &amp;quot;big picture&amp;quot;. (Which is true for each layered system btw.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The tradeoff for encapsulation is mapping - not only mapping the defined contract in and out the data access but also the consumer will have to do the same to its own representation. This truly shines when storage model differs drastically from business models but when they are alike it becomes a burden since there is little benefit to be seen.
I have observed a few times that in the early stages of system implementation domain and storage entities are 1:1 thus mapping only gets in the way and gets rejected because it &amp;quot;slows down the work&amp;quot;. It is only after the system goes live and becomes successful when subtle domain problems have to be solved - then the domain model starts to deviate from the storage model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the system is mostly &lt;a href="https://en.wikipedia.org/wiki/Create,_read,_update_and_delete"&gt;CRUD&lt;/a&gt;-ish this can turn to annoyance very quickly - it is even not applicable from a practical point of view in such a case.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So what if &amp;quot;Full Encapsulation&amp;quot; does not work for you - maybe your system won't benefit from it or maybe you are working in a context of microservice where deliberately reducing the levels of indirection?&lt;/p&gt;
&lt;h2 id="orm-without-abstraction"&gt;ORM Without Abstraction&lt;/h2&gt;
&lt;p&gt;Let's suppose data access is mostly about talking to a database. Then using &lt;a href="https://en.wikipedia.org/wiki/Object-relational_mapping"&gt;ORM&lt;/a&gt; framework makes sense in most cases. The big question here is whether putting an abstraction layer over another abstraction is necessary? The ORM will probably already implement the patterns you need like &lt;a href="https://www.martinfowler.com/eaaCatalog/identityMap.html"&gt;Identity Map&lt;/a&gt;, Unit Of Work, Specification, etc. and building abstraction over it will probably mimic the framework of choice. On the other hand, letting a dependency on a framework deeply in domain implementation is not a light decision to take. It is definitely worth thinking it through.&lt;/p&gt;
&lt;p&gt;Personally, I will bite the bullet and use the ORM directly without trying to abstract it. In this way, I will be able to use it to the full extent without restricting myself to an abstraction of common ORM features. I will still try to keep data access separate from business logic as much as possible. I can take comfort in the fact that I have an abstraction for the popular relational databases out there (if I ever need to switch). Databases and their supporting technologies have long lifecycles (compared to frontend ones for example). Most of them have reached maturity and will stay around for the foreseeable future.&lt;/p&gt;
&lt;p&gt;The testing strategy is not so clear in this case. I will mock the ORM for unit tests and try to extract business logic so it can be tested separately. Some will also unit test data access by making the ORM work with in-memory data set - I don't think there is any value in it because there is too much difference with the real thing. Integration tests for the data access are the way to go, though if there is a lot of mixing with business logic there no clear cut what each test should cover. Some will argue that data access is also business logic but I guess it depends on the point of view.&lt;/p&gt;
&lt;p&gt;There are cases when this will not be possible - company policies, strong opinions, or something already in place.&lt;/p&gt;
&lt;h2 id="repository-pattern"&gt;Repository Pattern&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://www.martinfowler.com/eaaCatalog/repository.html"&gt;Repository Pattern&lt;/a&gt; is the most popular approach for abstracting data access. It is also the most controversial. In my opinion, its flaw is that it tries to oversimplify data access by pretending it is an in-memory collection. I often dislike that it is built around a single entity and depending on implementation can enforce the same interface for all entities.&lt;/p&gt;
&lt;p&gt;There are some nuances in implementations worth noting:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Usually, the Repository will be accompanied by Unit of Work and Specification implementations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Persistence ignorance vs persistence awareness: basically the choice whether the consumer does not care about persistence details and will let infrastructure deal with saving, transaction handling, etc. or the consumer will have to explicitly manage them (i.e. call &lt;code&gt;Save()&lt;/code&gt; method).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Level of abstraction when using ORM: the promise of ORM is that it will let you persist your business/domain objects in the database seamlessly. In practice this is not happening - business objects are now &amp;quot;serving two masters&amp;quot; and they have to comply with the rules of the ORM which in turn can contradict the domain goals (mainly structuring and encapsulation). Another problem is that ORM abstractions are &amp;quot;leaky&amp;quot;. An example from &lt;a href="https://docs.microsoft.com/en-us/ef"&gt;Entity Framework&lt;/a&gt; - the concept navigation property. You never know whether it was eagerly loaded and there is no data, it is not loaded and you have to do it explicitly or it will be lazy loaded given lazy loading is enabled. And how to translate &lt;code&gt;.Include()&lt;/code&gt; as a meaningful domain concept? Another example is &lt;code&gt;.AsNoTracking()&lt;/code&gt; - how can your domain know that an entity is obtained in a special way so that changes to it will not be persisted? One solution to this is to have separate business objects from database entities and provide a mapping between them (or even introduce intermediary DTOs). This will make the domain &amp;quot;pure&amp;quot; but unfortunately, it adds additional complexity and as described in &amp;quot;Full Encapsulation&amp;quot; it can be quite a burden for simple cases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Usually, the Repository implementation will cover most of the cases. For the non-trivial cases (i.e. batch processing, weird queries) I would introduce separate abstractions instead of trying to fit everything in the Repository implementation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="generic-repository-implementation"&gt;Generic Repository Implementation&lt;/h3&gt;
&lt;p&gt;If a generic implementation is feasible/possible with the tools at hand I would go for it. Implementation will probably be shaped around the underlying framework since it will probably support generics too. Unit of Work, Specification and a way of defining projections (get only the data that you'll need) are mandatory for successful usage. Usually, such implementations are quite compact and will save a lot of coding. Some will argue that this is too much of generalization and it does not convey meaningful domain concepts - I tend to agree with this to some extent.&lt;/p&gt;
&lt;p&gt;I prefer using generic repository with domain services, each service &amp;quot;orchestrating&amp;quot; execution of specific use case (or domain concept). It will build specifications declaring the data it needs, delegate execution to the repository, then use the result to apply business logic. It can also dispatch the execution of complex domain logic to specialized classes.&lt;/p&gt;
&lt;p&gt;This approach makes very readable and maintainable use case implementations. The declarative nature of specifications leads to service &amp;quot;owning&amp;quot; it's data access. The ability to create and reuse named specifications allows defining consistent and meaningful definition of queries. Projections give more insight into what data exactly is needed for a particular case.&lt;/p&gt;
&lt;p&gt;Considerations for testing are pretty much the same as if ORM is used directly. We can go one step further by unit testing the Specifications ensuring we use declare correct input. This will not replace integration tests though.&lt;/p&gt;
&lt;h3 id="non-generic-repository-implementation"&gt;Non-Generic Repository Implementation&lt;/h3&gt;
&lt;p&gt;Non-generic repository implementation can become really close to &amp;quot;Full Encapsulation&amp;quot;. Unfortunately, the same downsides are valid also. The number of abstractions and implementations will grow in time. I would recommend figuring out some form of Specification (which appears to be a recurring theme in this post). Otherwise, you'll end up with methods clustered around some entity where each case will have its own method. I have seen such abstractions with more than 30 methods having absurd names like &lt;code&gt;GetOrdersOverTotalPriceTresholdForPeriodForCustomerForProductIncludingShippingInfoAndInvoiceNumber&lt;/code&gt;. Even worse is a method with a more general name having 15 parameters with 12 of them optional. At some point no one will look at them and will just add another method or parameter that he needs leading to massive code duplications.&lt;/p&gt;
&lt;p&gt;Implemented with care non-generic repository can become a good abstraction. Testing is also clear - mocks for repositories and tests for specifications in unit tests, integration tests for implementations.&lt;/p&gt;
&lt;h2 id="more"&gt;More...&lt;/h2&gt;
&lt;p&gt;There are more things to be considered for data access, which usually come up along the way:
batch processing, caching, performance, transactions, compensating actions, concurrency issues, retry policies to name a few. I have not touched upon &lt;a href="https://martinfowler.com/eaaDev/EventSourcing.html"&gt;Event Sourcing&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Command%E2%80%93query_separation#Command_query_responsibility_segregation"&gt;CQRS&lt;/a&gt; since I look at them as a bit more specialized solutions to specific problems.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;If you are more confused now than before reading this post it is probably a good thing. Data access should be an implementation detail but it does not mean it will be easy. The only advice I can give is to take a pragmatic approach, not a dogmatic one. Do what works best for you, your team, and the software you build. And if something does not work well - don't be afraid to change it.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;Best practices in software development dictate that data storage and retrieval should be treated as an implementation detail. In practice it is easier said than done though - there are endless debates about how much abstraction is needed, whether the Repository Pattern is still relevant and if yes how to implement it, are ORMs good or evil, Micro ORMs and so on.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>https://pmarinov.xyz/posts/2020-03-22-breaking-change-in-entity-framework-core-provider-for-postgresql-3</id>
		<title>Breaking change in Entity Framework Core provider for PostgreSQL 3.0</title>
		<link href="https://pmarinov.xyz/posts/2020-03-22-breaking-change-in-entity-framework-core-provider-for-postgresql-3" />
		<updated>2020-03-22T00:00:00Z</updated>
		<content>&lt;!-- # Breaking change in Entity Framework Core provider for PostgreSQL 3.0 --&gt;
&lt;p&gt;Beware when upgrading form &lt;code&gt;Npgsql.EntityFrameworkCore.PostgreSQL 2.2&lt;/code&gt; and running &lt;code&gt;PostgreSQL 9.6&lt;/code&gt; or earlier.&lt;/p&gt;
&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;Recently I wanted to add &lt;a href="https://docs.microsoft.com/en-us/aspnet/core/security/authentication/identity?view=aspnetcore-3.1&amp;amp;tabs=visual-studio"&gt;ASP.NET Core Identity&lt;/a&gt; to an application using PostgreSQL as a data storage. Since I prefer to manage database schema and migrations separately from the application (currently using &lt;a href="https://dbup.readthedocs.io/en/latest/"&gt;DbUp&lt;/a&gt;) I decided to use Entity Framework Core migrations to generate the SQL script for it.&lt;/p&gt;
&lt;h2 id="generating-the-migration-sql-script"&gt;Generating the migration SQL script&lt;/h2&gt;
&lt;p&gt;Using the Entity Framework Core tools 3.x is straight-froward.
(Given that you have added the necessary dependencies and defined your application's &lt;code&gt;IdentityDbContext&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;First make sure it is installed:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;dotnet tool install --global dotnet-ef
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Initialize the migrations in the project where your DbContext is:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;dotnet ef migrations add init
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then generate the SQL script for the DbContext (called ApplicationDbContext in my case):&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;dotnet ef migrations script -c ApplicationDbContext -o migration_script.sql
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Removed the SQL related to &lt;code&gt;__EFMigrationsHistory&lt;/code&gt; and my migration script was ready.
Then removed the migrations folder from my project and was ready to go.&lt;/p&gt;
&lt;p&gt;Ran the SQL script and... it failed!&lt;/p&gt;
&lt;h2 id="the-issue"&gt;The issue&lt;/h2&gt;
&lt;p&gt;I was quite surprized to see that the error was in the SQL:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ERROR: syntax error at or near &amp;quot;GENERATED&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the offending statement was:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;CREATE TABLE &amp;quot;AspNetRoleClaims&amp;quot; (
    &amp;quot;Id&amp;quot; integer NOT NULL GENERATED BY DEFAULT AS IDENTITY,
...    
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which on further research made perfect sense because this is a new syntax  &lt;code&gt;GENERATED AS IDENTITY&lt;/code&gt; for creating auto-incremented column available in PostgreSQL 10 and I was running version 9.6.&lt;/p&gt;
&lt;p&gt;I was using &lt;code&gt;Npgsql.EntityFrameworkCore.PostgreSQL 3.1.1&lt;/code&gt; and going through it I found a documented breaking change in &lt;code&gt;3.0.0&lt;/code&gt;:&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;em&gt;The default value generation strategy has changed from the older SERIAL columns to the newer IDENTITY columns, introduced in PostgreSQL 10.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can look for details here: &lt;a href="https://www.npgsql.org/efcore/release-notes/3.0.html"&gt;3.0 Release Notes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fortunately the fix was easy - just had to specify the PostgreSQL version I was targeting:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class ApplicationDbContext : IdentityDbContext
{
    public ApplicationDbContext(DbContextOptions options) : base(options)
    {
    }

    protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder)
    {
        optionsBuilder.UseNpgsql(&amp;quot;DefaultConnection&amp;quot;, o =&amp;gt; o.SetPostgresVersion(9, 6));
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After re-generating the migrations the older &lt;code&gt;SERIAL&lt;/code&gt; statement was used:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;CREATE TABLE &amp;quot;AspNetRoleClaims&amp;quot; (
    &amp;quot;Id&amp;quot; serial NOT NULL,
...    
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and everything went fine.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;PostgreSQL 10 introduced &lt;code&gt;GENERATED AS IDENTITY&lt;/code&gt; syntax aiming to replace &lt;code&gt;SERIAL&lt;/code&gt; for automatic assignment of unique value to a column. &lt;code&gt;Npgsql.EntityFrameworkCore.PostgreSQL 3.0.0&lt;/code&gt; takes advantage of it and uses it as a default when generating SQL migrations. Upgrading dependencies caught me off guard this time but the breaking change was well documented and my problem easily fixed.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;Beware when upgrading form &lt;code&gt;Npgsql.EntityFrameworkCore.PostgreSQL 2.2&lt;/code&gt; and running &lt;code&gt;PostgreSQL 9.6&lt;/code&gt; or earlier.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>https://pmarinov.xyz/posts/2020-02-27-deleting-large-amounts-of-data-from-ms-sql-server-database</id>
		<title>Deleting large amounts of data from MS SQL Server database</title>
		<link href="https://pmarinov.xyz/posts/2020-02-27-deleting-large-amounts-of-data-from-ms-sql-server-database" />
		<updated>2020-02-27T00:00:00Z</updated>
		<content>&lt;!--- # Deleting large amounts of data from MS SQL Server database ---&gt;
&lt;p&gt;Deleting a large number of rows (like millions of them) from a table has a downside of being slow and making a transaction log file to explode in terms of size. Here is an approach that worked for me to overcome these obstacles.&lt;/p&gt;
&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;My problem was that in a multi-tenant database a tenant has grown too much - to the point that it had to be moved to its own database. The plan was to restore a backup of the database and delete all other tenant's data. The database had around 150 tables - for most deletions were not a problem - but for around 20 of them more than 10 million rows were to be deleted and for particular 5 tables more than 600 million.&lt;/p&gt;
&lt;h2 id="first-iteration-standard-deletions"&gt;First iteration: Standard deletions&lt;/h2&gt;
&lt;p&gt;For the first version of the deletion, I used standard delete statements similar to:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;DELETE x FROM a_table x JOIN &amp;#64;TenantsToDelete t ON t.Id=x.TenantId; 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It worked fine on a small data set and was useful to confirm the deletion order and flush out some bad data and design oddities. Also, it has no problem for a few thousand to even tens of thousands of rows so it worked fine for the majority of the cases.&lt;br /&gt;
Where millions of rows were to be deleted it was taking a long time and transaction log was growing rapidly. The full deletion script actually never ran to completion on production-grade data.&lt;/p&gt;
&lt;h2 id="second-iteration-batched-deletions"&gt;Second iteration: Batched deletions&lt;/h2&gt;
&lt;p&gt;For the large tables I tried batched deletes similar to:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;DECLARE &amp;#64;batch_size INT=5000;
delete_top:
  DELETE TOP(&amp;#64;batch_size) x FROM a_table x JOIN &amp;#64;TenantsToDelete t ON t.Id=x.TenantId;
  IF &amp;#64;batch_size = &amp;#64;&amp;#64;ROWCOUNT GOTO delete_top;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This version of the script was able to complete the deletion of a single small-sized tenant for around 30 minutes, for a large one it took more than 15 hours - and I needed to remove around 40 of them. It was still no good.&lt;/p&gt;
&lt;h2 id="third-iteration-replace-delete-with-insert"&gt;Third iteration: Replace DELETE with INSERT&lt;/h2&gt;
&lt;p&gt;So &lt;code&gt;DELETE&lt;/code&gt; is slow and there is no much room for improvement. There is no &lt;em&gt;&amp;quot;bulk delete&amp;quot;&lt;/em&gt; kind of operation in MS SQL Server to boost the performance and I was not in a position to use &lt;code&gt;TRUNCATE&lt;/code&gt; or &lt;code&gt;DROP TABLE&lt;/code&gt; since I needed to preserve part of the data.&lt;/p&gt;
&lt;p&gt;While searching for a solution I found this article about optimizing the loading of data and the impact of minimally logged operations on I/O: &lt;a href="https://docs.microsoft.com/en-us/previous-versions/sql/sql-server-2008/dd425070(v=sql.100)?redirectedfrom=MSDN"&gt;The Data Loading Performance Guide&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In essence, we can insert data fast (with minimal logging) when these conditions are met:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the database is in &lt;code&gt;SIMPLE&lt;/code&gt; or &lt;code&gt;BULK_LOGGED&lt;/code&gt; recovery mode,&lt;/li&gt;
&lt;li&gt;the target table is a &lt;em&gt;heap table&lt;/em&gt; (without a clustered index),&lt;/li&gt;
&lt;li&gt;a &lt;code&gt;WITH (TABLOCK)&lt;/code&gt; hint is used with the insert (allows exclusive lock on the target table).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is an additional performance boost using the latter in SQL Server 2016 and higher - &lt;code&gt;INSERT...SELECT WITH (TABLOCK)&lt;/code&gt; may use parallel inserts.&lt;/p&gt;
&lt;p&gt;So back to my deletion problem - what if instead of delete I do the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a heap table with exactly the same structure as the one I want to delete from&lt;/li&gt;
&lt;li&gt;Move to the heap table data I want to &lt;strong&gt;keep&lt;/strong&gt; using &lt;code&gt;INSERT...SELECT WITH (TABLOCK)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Drop the original table (metadata-only operation, fast)&lt;/li&gt;
&lt;li&gt;Rename the heap table to original one's name&lt;/li&gt;
&lt;li&gt;Create indexes, constraints, references, etc.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;TL;DR;&lt;/strong&gt;&lt;/em&gt; Yes, it worked and it was way faster without growing the transaction log since all operations were minimally logged.&lt;br /&gt;
Replacing the deletion of my around 30 problematic tables with this technique lead to deletion of everything but a mid-sized tenant data to complete in 15 minutes.&lt;br /&gt;
Deletion of everything but the largest tenant (moving the most data) completed in 1 hour.&lt;/p&gt;
&lt;h3 id="generating-the-sql"&gt;Generating the SQL&lt;/h3&gt;
&lt;p&gt;Although I was quite happy with the performance, there was another problem - scripting the heap tables, moving the data, and re-creating the indexes by hand is tedious and error-prone (imagine doing it for 30 tables).&lt;/p&gt;
&lt;p&gt;SQL Server Management Studio has a lot of scripting capabilities and fortunately, they are available via the &lt;a href="https://docs.microsoft.com/en-us/sql/relational-databases/server-management-objects-smo/sql-server-management-objects-smo-programming-guide?view=sql-server-ver15"&gt;SQL Management Objects&lt;/a&gt; (SMO). It is a set of .NET Framework assemblies meaning that they can be used from PowerShell also.&lt;/p&gt;
&lt;p&gt;Loading the assemblies and creating &lt;code&gt;Microsoft.SqlServer.Management.Smo.Server&lt;/code&gt; instance is the first step:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;[System.Reflection.Assembly]::LoadWithPartialName(&amp;quot;Microsoft.SqlServer.Smo&amp;quot;)
[System.Reflection.Assembly]::LoadWithPartialName(&amp;quot;Microsoft.SqlServer.ConnectionInfo&amp;quot;)

$connection = New-Object System.Data.SqlClient.SqlConnection $sqlConnectionString
$serverConnection = New-Object Microsoft.SqlServer.Management.Common.ServerConnection $connection

[Microsoft.SqlServer.Management.Smo.Server] $server = New-Object Microsoft.SqlServer.Management.Smo.Server $serverConnection
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To create a heap table:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;$scripter = New-Object Microsoft.SqlServer.Management.Smo.Scripter $server

$script = $scripter.Script(($server.Databases[$connection.Database].Tables[$table]))

# scripter can only generate scripts for existing objects so we need to change the table name 
$script[2] = $script[2].Replace(&amp;quot;CREATE TABLE [dbo].[$table]&amp;quot;, &amp;quot;CREATE TABLE [dbo].[$heapTable]&amp;quot;)
$script
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For moving the data, we can generate the columns list to use in the insert statement:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;$columnNames = $server.Databases[$connection.Database].Tables[$table].Columns | Select-Object $_ | ForEach-Object {$_.Name}
$columns = [System.String]::Join(', ', $columnNames)

$sql = New-Object System.Collections.Specialized.StringCollection
$sql.Add(&amp;quot;INSERT INTO $heapTable WITH(TABLOCK)&amp;quot;) &amp;gt; $null
$sql.Add(&amp;quot;($columns)&amp;quot;) &amp;gt; $null
$sql.Add(&amp;quot;SELECT&amp;quot;) &amp;gt; $null
$sql.Add(&amp;quot;$columns FROM $table &amp;quot;) &amp;gt; $null
$sql.Add(&amp;quot;$whereClause&amp;quot;) &amp;gt; $null

$sql
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are some pitfalls to consider:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;beware of &lt;code&gt;TIMESTAMP&lt;/code&gt; columns: you cannot insert them&lt;/li&gt;
&lt;li&gt;beware of &lt;code&gt;IDENTITY&lt;/code&gt; columns: I found it hard to change column to identity so create it as an identity and use &lt;code&gt;SET IDENTITY_INSERT ON/OFF&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;again for &lt;code&gt;IDENTITY&lt;/code&gt; columns: the scripter will capture the current identity seed when generating the script but it will be executed at some later point and the seed will be different. Thus in my script I capture the seed from the original table column after the data is moved and reseed the column of the heap table. To find the identity column you can use something like:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;$server.Databases[$connection.Database].Tables[$table].Columns | Where-Object { $_.Identity -eq $true } | Select-Object -First 1
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;depending on your &lt;code&gt;$whereClause&lt;/code&gt; you may need to prefix the columns in the select list, e.g. when using something like &lt;code&gt;SELECT x.ID FROM x_Table x JOIN y_Table y on y.Id=x.Id...&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before dropping the original table we need to remove all references to it's Primary Key:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;$inboundForeignKeys = $server.Databases[$connection.Database].Tables[$table].EnumForeignKeys()
foreach($foreignKey in $inboundForeignKeys)
{
    Write-Output &amp;quot;ALTER TABLE [dbo].[$($foreignKey['Table_Name'])] DROP CONSTRAINT [$($foreignKey['Name'])];&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you have triggers or schema-bound objects or anything preventing table to be drop should be taken care of - luckily in my case there was not.
Then we can drop the original table and rename the heap table. Nothing special required - &lt;code&gt;DROP TABLE&lt;/code&gt; and &lt;code&gt;sp_rename&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The last step is to create primary and foreign keys, defaults, constraints and indexes:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;# indexes, including primary key
$scripter = New-Object Microsoft.SqlServer.Management.Smo.Scripter $server
$scripter.Options.NoCollation = $true
$scripter.Options.DriPrimaryKey = $true
$scripter.Options.DriUniqueKeys = $true
$scripter.Options.ClusteredIndexes = $true
$scripter.Options.NonClusteredIndexes = $true
$scripter.Options.Indexes = $true
$scripter.Options.DriAll = $true

[Microsoft.SqlServer.Management.Smo.SqlSmoObject[]]$indexes = $server.Databases$connection.Database].Tables[$table].Indexes

$scripter.Script($indexes)

# default constraints
$columns = $server.Databases[$connection.Database].Tables[$table].Columns
foreach($column in $columns)
{
    if($column.DefaultConstraint)
    {
        $column.DefaultConstraint.Script()
    }
}

# foreign keys
[Microsoft.SqlServer.Management.Smo.SqlSmoObject[]]$foreignKeys = $server.Databases[$connection.Database].Tables[$table].ForeignKeys

$scripter.Script($foreignKeys)

# foreign keys referencing the table
$inboundForeignKeys = $server.Databases[$connection.Database].Tables[$table].EnumForeignKeys()
foreach($foreignKey in $inboundForeignKeys)
{
    [Microsoft.SqlServer.Management.Smo.SqlSmoObject[]]$key = $server.Databases[$connection.Database].Tables[$foreignKey['Table_Name']].ForeignKeys[$foreignKey['Name']]

    $scripter.Script($key)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are the most important pieces I needed to generate the SQL.&lt;/p&gt;
&lt;p&gt;In practice, I turned the initial deletion SQL script into a kind of template containing the standard deletion statements and occasionally a placeholder with the table name and where clause defining the data to be kept. Then a PowerShell script will read the template and replace the placeholders with SQL statements generated using the steps described above.&lt;/p&gt;
&lt;p&gt;Having automated SQL script generation based on the actual metadata has two main advantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;removes the repetitive (and boring) manual work&lt;/li&gt;
&lt;li&gt;when the schema changes (e.g. a table is added or altered, a column is changed or dropped, an index is created, etc.) the SQL script can be easily generated again&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Replacing regular SQL deletes with inserting in heap table and dropping the original one has achieved a satisfying performance boost. It is a combination of SQL Server's data loading capabilities and minimally logged operations. In my case, it worked well since I needed to keep (move) 20-30% of the table's data.&lt;/p&gt;
&lt;p&gt;Implementation is more complex than regular deletes so an automated SQL script generation comes handy.&lt;/p&gt;
&lt;p&gt;The downside of the solution is it's complexity so I would recommend it only as a last resort.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;Deleting a large number of rows (like millions of them) from a table has a downside of being slow and making a transaction log file to explode in terms of size. Here is an approach that worked for me to overcome these obstacles.&lt;/p&gt;</summary>
	</entry>
</feed>