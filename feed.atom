<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
	<id>https://pmarinov.xyz/</id>
	<title>Petar Marinov</title>
	<link rel="self" href="https://pmarinov.xyz/" />
	<rights>2020</rights>
	<updated>2020-05-03T15:58:30Z</updated>
	<subtitle>/dev/shared</subtitle>
	<entry>
		<id>https://pmarinov.xyz/posts/2020-05-03-structuring-data-dccess</id>
		<title>Structuring Data Access</title>
		<link href="https://pmarinov.xyz/posts/2020-05-03-structuring-data-dccess" />
		<updated>2020-05-03T00:00:00Z</updated>
		<content>&lt;!-- # Structuring Data Access --&gt;
&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;Best practices in software development dictate that data storage and retrieval should be treated as an implementation detail. In practice it is easier said than done though - there are endless debates about how much abstraction is needed, whether the Repository Pattern is still relevant and if yes how to implement it, are ORMs good or evil, Micro ORMs and so on.&lt;/p&gt;
&lt;p&gt;Here are some solutions with their strengths and weaknesses.&lt;/p&gt;
&lt;h2 id="full-encapsulation"&gt;Full Encapsulation&lt;/h2&gt;
&lt;p&gt;&amp;quot;Full Encapsulation&amp;quot; is inspired by Uncle Bob's &lt;a href="https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html"&gt;Clean Architecture&lt;/a&gt; - have an abstraction with simple data structures going in and out. Internally the data access implementation will map these simple data structures to storage entities which suit best the libraries or frameworks used. In terms of total decoupling from any storage technology this is the best approach. It's strict abstraction gives a lot of flexibility - i.e. a team can work on the data access details or multiple data access implementations can be switched or &lt;a href="https://en.wikipedia.org/wiki/Polyglot_persistence"&gt;Polyglot Persistence&lt;/a&gt; strategy can be utilized. Some implementations guidelines:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Organize abstractions and implementations to be in different assemblies (packages, modules) (&lt;a href="https://www.martinfowler.com/eaaCatalog/separatedInterface.html"&gt;Separated interface&lt;/a&gt;). I have often seen both in the same place - from practical point of view this is not a bad thing but if you think about different implementation you will need to depend on all the concrete stuff along with the abstractions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Prefer persistence ignorant abstractions (i.e. do not expose a &lt;code&gt;Save()&lt;/code&gt; method) - each is atomic operation by itself. It is possible to have multiple abstractions to act as atomic action by introducing a variation of &lt;a href="https://www.martinfowler.com/eaaCatalog/unitOfWork.html"&gt;Unit Of Work&lt;/a&gt;. Personally I will not go this way since it introduces significant amount of complexity - individual abstractions will loose their autonomy and probably will have implicit dependencies, additional authority (the Unit of Work) should be managed and ultimately it will complicate usage from consumer perspective.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Consider some form of &lt;a href="https://en.wikipedia.org/wiki/Specification_pattern"&gt;Specification&lt;/a&gt; for retrieving data. It does not have to be a strict implementation of the pattern, rather it's intent is to provide a way for declaring some aspect of the data we are interested in. This will simply allow for grouping similar data access patterns together i.e. if we have orders we can specify time range, customer, product, etc. I would not go too far with this trying to define every possible case. Instead when complexity grows beyond certain point I would split in different abstractions i.e. customer's orders, product's orders, etc. - each having appropriate &amp;quot;specification&amp;quot;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There is no need the abstraction to be about a single entity - it can represent a series of actions serving specific business case, i.e. storing related data in multiple database tables or pulling data from multiple sources to generate a report.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Prefer having a single public method. This will keep the focus on single functionality and make it simple to use.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sometimes having abstraction for input and output data may be an overkill - than &lt;a href="https://en.wikipedia.org/wiki/Data_transfer_object"&gt;DTOs&lt;/a&gt; can be used instead. In both cases I would not add any behavior to them.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Working on the data access in this setup is absolute joy - being shielded from other parts of the system gives you freedom. You can use whatever libraries/frameworks, technologies and data sources you need. This is the place where choice actually matters - you will choose a technology/framework/library because you'll pit to use it's unique features. At the same time the &lt;a href="https://en.wikipedia.org/wiki/Leaky_abstraction"&gt;&amp;quot;leakiness&amp;quot;&lt;/a&gt; of the abstraction can be minimal. Testing strategy is also clear - integration tests for the implementation and mocks for unit tests.&lt;/p&gt;
&lt;p&gt;Unfortunately such strict abstraction also has it's downsides:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The number of abstractions, implementations and mappings will grow in time since usually each business case will become to have it's own unique requirements. Striking the balance between reusability and simplicity may become very tricky and hard to recognize.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A business case will become &amp;quot;stretched&amp;quot; through layers which may lead to having hard time investigating issues or trying to get the &amp;quot;big picture&amp;quot;. (Which is true for each layered system btw.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The tradeoff for encapsulation is mapping - not only mapping the defined contract in and out the data access but also the consumer will have to do the same to it's own representation. This truly shines when storage model differs drastically from business models but when they are alike it becomes a burden since there is little benefit to be seen.
I have observed a few times that in early stages of system implementation domain and storage entities are 1:1 thus mapping only gets in the way and gets rejected because it &amp;quot;slows down the work&amp;quot;. It is only after the system goes live and becomes successful when subtle domain problems have to be solved - then domain model starts to deviate from the storage model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the system is mostly &lt;a href="https://en.wikipedia.org/wiki/Create,_read,_update_and_delete"&gt;CRUD&lt;/a&gt;-ish this can turn to annoyance very quickly - imo it is even not applicable from practical point of view in such case.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So what if &amp;quot;Full Encapsulation&amp;quot; does not work for you - maybe your system won't benefit from it or maybe you are working in a context of microservice where deliberately reducing the levels of indirection?&lt;/p&gt;
&lt;h2 id="orm-without-abstraction"&gt;ORM Without Abstraction&lt;/h2&gt;
&lt;p&gt;Let's suppose data access is mostly about talking to a database. Then using &lt;a href="https://en.wikipedia.org/wiki/Object-relational_mapping"&gt;ORM&lt;/a&gt; framework makes sense in most cases. The big question here is whether putting an abstraction layer over another abstraction is necessary? The ORM will probably already implement the patterns you need like &lt;a href="https://www.martinfowler.com/eaaCatalog/identityMap.html"&gt;Identity Map&lt;/a&gt;, Unit Of Work, Specification, etc. and building abstraction over it will probably mimic the framework of choice. On the other hand letting a dependency on a framework deeply in domain implementation is not a light decision to take. It is definitely worth thinking it through.&lt;/p&gt;
&lt;p&gt;Personally I will byte the bullet and use the ORM directly without trying to abstract it. In this way I will be able to use it to full extent without restricting myself to abstraction of common ORM features. I will still try to keep data access separate from business logic as much as possible. I can take comfort in the fact that I have an abstraction for the popular relational databases out there (if I ever need to switch). Databases and their supporting technologies have long lifecycles (compared to frontend ones for example). Most of them have reached maturity and will stay around for the foreseeable future.&lt;/p&gt;
&lt;p&gt;Testing strategy is not so clear in this case. I will mock the ORM for unit tests and try to extract business logic so it can be tested separately. Some will also unit test data access by making the ORM work with in-memory data set - personally I don't think there is any value in it because there is too much difference with the real thing. Integration tests for the data access are the way to go, though if there is a lot of mixing with business logic there no clear cut what each test should cover. Some will argue that data access is also business logic but I guess it depends on the point of view.&lt;/p&gt;
&lt;p&gt;There are cases when this will not be possible though - company policies, strong opinions or something already in place.&lt;/p&gt;
&lt;h2 id="repository-pattern"&gt;Repository Pattern&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://www.martinfowler.com/eaaCatalog/repository.html"&gt;Repository Pattern&lt;/a&gt; is the most popular approach for abstracting data access. It is also the most controversial. In my opinion its flaw is that it tries to oversimplify data access by pretending it is an in-memory collection. I often dislike that it is built around single entity and depending on implementation can enforce same interface for all entities.&lt;/p&gt;
&lt;p&gt;There are some nuances in implementations worth noting:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Usually the Repository will be accompanied by Unit of Work and Specification implementations.&lt;/li&gt;
&lt;li&gt;Persistence ignorance vs persistence awareness: basically the choice whether the consumer does not care about persistence details and will let infrastructure deal with saving, transactions handling, etc. or the consumer will have to explicitly manage them (i.e. call &lt;code&gt;Save()&lt;/code&gt; method).&lt;/li&gt;
&lt;li&gt;Level of abstraction when using ORM: the promise of ORM is that it will let you persist your business/domain objects in database seamlessly. In practice this is not happening - business objects are now &amp;quot;serving two masters&amp;quot; and they have to comply with the rules of the ORM which in turn can contradict the domain goals (mainly structuring and encapsulation). Another problem is that ORM abstractions are &amp;quot;leaky&amp;quot;. An example from &lt;a href="https://docs.microsoft.com/en-us/ef"&gt;Entity Framework&lt;/a&gt; - the concept navigation property. You never know whether it was eagerly loaded and there is no data, it is not loaded and you have to do it explicitly or it will be lazy loaded given lazy loading is enabled. And how to translate &lt;code&gt;.Include()&lt;/code&gt; as a meaningful domain concept? Another example is the &lt;code&gt;.AsNoTracking()&lt;/code&gt; - how can your domain know that an entity is obtained in a special way so that changes to it will not be persisted? One solution to this is to have separate business objects from database entities and provide mapping between them (or even introduce intermediary DTOs). This will make the domain &amp;quot;pure&amp;quot; but unfortunately it adds additional complexity and as described in &amp;quot;Full Encapsulation&amp;quot; it can be quite a burden for simple cases.&lt;/li&gt;
&lt;li&gt;Usually the Repository implementation will cover the most of the cases. For the non-trivial cases (i.e. batch processing, weird queries) I would introduce separate abstractions instead of trying fit everything in the Repository implementation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="generic-repository-implementation"&gt;Generic Repository Implementation&lt;/h3&gt;
&lt;p&gt;If generic implementation is feasible/possible with the tools at hand I would go for it. Implementation will probably be shaped around the underlying framework since it will probably support generics too. Unit of Work, Specification and a way of defining projections (get only the data that you'll need) are mandatory for successful usage. Usually such implementations are quite compact and will save a lot of coding. Some will argue that this is too much of generalization and it does not convey meaningful domain concepts - I tend to agree with this to some extent.&lt;/p&gt;
&lt;p&gt;I prefer using generic repository with domain services, each service &amp;quot;orchestrating&amp;quot; execution of specific use case (or domain concept). It will build specifications declaring the data it needs, delegate execution to the repository, then use the result to apply business logic. It can also dispatch the execution of complex domain logic to specialized classes.&lt;/p&gt;
&lt;p&gt;This approach makes very readable and maintainable use case implementations. The declarative nature of specifications leads to service &amp;quot;owning&amp;quot; it's data access. The ability to create and reuse named specifications allows defining of consistent and meaningful definition of queries. Projections give more insight of what data exactly is needed for a particular case.&lt;/p&gt;
&lt;p&gt;Consideration for testing are pretty much the same as if ORM is used directly. We can go one step further by unit testing the Specifications ensuring we use declare correct input. This will not replace integration test though.&lt;/p&gt;
&lt;h3 id="non-generic-repository-implementation"&gt;Non Generic Repository Implementation&lt;/h3&gt;
&lt;p&gt;Non generic repository implementation can become really close to &amp;quot;Full Encapsulation&amp;quot;. Unfortunately the same downsides are valid also. The number of abstractions and implementations will grow in time. I would recommend figuring out some form of Specification (which appears to be recurring theme in this post). Otherwise you'll end up with methods clustered around some entity where each case will have it's own method. I have seen such abstractions with more than 30 methods having absurd names like &lt;code&gt;GetOrdersOverTotalPriceTresholdForPeriodForCustomerForProductIncludingShippingInfoAndInvoiceNumber&lt;/code&gt;. Even worse is a method with more general name having 15 parameters with 12 of them optional. At some point no one will look at them and will just add another method or parameter that he needs leading to massive code duplications.&lt;/p&gt;
&lt;p&gt;Implemented with care non generic repository can become a good abstraction. Testing is also clear - mocks for repositories and tests for specifications in unit tests, integration tests for implementations.&lt;/p&gt;
&lt;h2 id="more"&gt;More...&lt;/h2&gt;
&lt;p&gt;There are more things to be considered for data access, which usually come up along the way:
batch processing, caching, performance, transactions, compensating actions, concurrency issues, retry policies to name a few. I have not touched upon &lt;a href="https://martinfowler.com/eaaDev/EventSourcing.html"&gt;Event Sourcing&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Command%E2%80%93query_separation#Command_query_responsibility_segregation"&gt;CQRS&lt;/a&gt; since I look at them as a bit more specialized solutions to specific problems.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;If you are more confused now than before reading this post it is probably a good thing. Data access should be an implementation detail but it does not mean it will be easy. The only advice I can give is to take a pragmatic approach, not a dogmatic one. Do what works best for you, your team and the software you build. And if something does not work well - don't be afraid to change it.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;Best practices in software development dictate that data storage and retrieval should be treated as an implementation detail. In practice it is easier said than done though - there are endless debates about how much abstraction is needed, whether the Repository Pattern is still relevant and if yes how to implement it, are ORMs good or evil, Micro ORMs and so on.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>https://pmarinov.xyz/posts/2020-03-22-breaking-change-in-entity-framework-core-provider-for-postgresql-3</id>
		<title>Breaking change in Entity Framework Core provider for PostgreSQL 3.0</title>
		<link href="https://pmarinov.xyz/posts/2020-03-22-breaking-change-in-entity-framework-core-provider-for-postgresql-3" />
		<updated>2020-03-22T00:00:00Z</updated>
		<content>&lt;!-- # Breaking change in Entity Framework Core provider for PostgreSQL 3.0 --&gt;
&lt;p&gt;Beware when upgrading form &lt;code&gt;Npgsql.EntityFrameworkCore.PostgreSQL 2.2&lt;/code&gt; and running &lt;code&gt;PostgreSQL 9.6&lt;/code&gt; or earlier.&lt;/p&gt;
&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;Recently I wanted to add &lt;a href="https://docs.microsoft.com/en-us/aspnet/core/security/authentication/identity?view=aspnetcore-3.1&amp;amp;tabs=visual-studio"&gt;ASP.NET Core Identity&lt;/a&gt; to an application using PostgreSQL as a data storage. Since I prefer to manage database schema and migrations separately from the application (currently using &lt;a href="https://dbup.readthedocs.io/en/latest/"&gt;DbUp&lt;/a&gt;) I decided to use Entity Framework Core migrations to generate the SQL script for it.&lt;/p&gt;
&lt;h2 id="generating-the-migration-sql-script"&gt;Generating the migration SQL script&lt;/h2&gt;
&lt;p&gt;Using the Entity Framework Core tools 3.x is straight-froward.
(Given that you have added the necessary dependencies and defined your application's &lt;code&gt;IdentityDbContext&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;First make sure it is installed:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;dotnet tool install --global dotnet-ef
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Initialize the migrations in the project where your DbContext is:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;dotnet ef migrations add init
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then generate the SQL script for the DbContext (called ApplicationDbContext in my case):&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;dotnet ef migrations script -c ApplicationDbContext -o migration_script.sql
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Removed the SQL related to &lt;code&gt;__EFMigrationsHistory&lt;/code&gt; and my migration script was ready.
Then removed the migrations folder from my project and was ready to go.&lt;/p&gt;
&lt;p&gt;Ran the SQL script and... it failed!&lt;/p&gt;
&lt;h2 id="the-issue"&gt;The issue&lt;/h2&gt;
&lt;p&gt;I was quite surprized to see that the error was in the SQL:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ERROR: syntax error at or near &amp;quot;GENERATED&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the offending statement was:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;CREATE TABLE &amp;quot;AspNetRoleClaims&amp;quot; (
    &amp;quot;Id&amp;quot; integer NOT NULL GENERATED BY DEFAULT AS IDENTITY,
...    
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which on further research made perfect sense because this is a new syntax  &lt;code&gt;GENERATED AS IDENTITY&lt;/code&gt; for creating auto-incremented column available in PostgreSQL 10 and I was running version 9.6.&lt;/p&gt;
&lt;p&gt;I was using &lt;code&gt;Npgsql.EntityFrameworkCore.PostgreSQL 3.1.1&lt;/code&gt; and going through it I found a documented breaking change in &lt;code&gt;3.0.0&lt;/code&gt;:&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;em&gt;The default value generation strategy has changed from the older SERIAL columns to the newer IDENTITY columns, introduced in PostgreSQL 10.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can look for details here: &lt;a href="https://www.npgsql.org/efcore/release-notes/3.0.html"&gt;3.0 Release Notes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fortunately the fix was easy - just had to specify the PostgreSQL version I was targeting:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class ApplicationDbContext : IdentityDbContext
{
    public ApplicationDbContext(DbContextOptions options) : base(options)
    {
    }

    protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder)
    {
        optionsBuilder.UseNpgsql(&amp;quot;DefaultConnection&amp;quot;, o =&amp;gt; o.SetPostgresVersion(9, 6));
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After re-generating the migrations the older &lt;code&gt;SERIAL&lt;/code&gt; statement was used:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;CREATE TABLE &amp;quot;AspNetRoleClaims&amp;quot; (
    &amp;quot;Id&amp;quot; serial NOT NULL,
...    
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and everything went fine.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;PostgreSQL 10 introduced &lt;code&gt;GENERATED AS IDENTITY&lt;/code&gt; syntax aiming to replace &lt;code&gt;SERIAL&lt;/code&gt; for automatic assignment of unique value to a column. &lt;code&gt;Npgsql.EntityFrameworkCore.PostgreSQL 3.0.0&lt;/code&gt; takes advantage of it and uses it as a default when generating SQL migrations. Upgrading dependencies caught me off guard this time but the breaking change was well documented and my problem easily fixed.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;Beware when upgrading form &lt;code&gt;Npgsql.EntityFrameworkCore.PostgreSQL 2.2&lt;/code&gt; and running &lt;code&gt;PostgreSQL 9.6&lt;/code&gt; or earlier.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>https://pmarinov.xyz/posts/2020-02-27-deleting-large-amounts-of-data-from-ms-sql-server-database</id>
		<title>Deleting large amounts of data from MS SQL Server database</title>
		<link href="https://pmarinov.xyz/posts/2020-02-27-deleting-large-amounts-of-data-from-ms-sql-server-database" />
		<updated>2020-02-27T00:00:00Z</updated>
		<content>&lt;!--- # Deleting large amounts of data from MS SQL Server database ---&gt;
&lt;p&gt;Deleting large amount of rows (like millions of them) from a table has a downside of being slow and making a transaction log file to explode in terms of size. Here is an approach that worked for me overcome these obstacles.&lt;/p&gt;
&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;My problem was that in a multi-tenant database a tenant has grown too much - to the point that it had to be moved to it's own database. The plan was to restore a backup of the database and delete all other tenant's data. The database had around 150 tables - for most of them deletions were not a problem - but for around 20 of them more than 10 million rows were to be deleted and for particular 5 tables more than 600 million.&lt;/p&gt;
&lt;h2 id="first-iteration-standard-deletions"&gt;First iteration: Standard deletions&lt;/h2&gt;
&lt;p&gt;For the first version of the deletion I used standard delete statements similar to:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;DELETE x FROM a_table x JOIN &amp;#64;TenantsToDelete t ON t.Id=x.TenantId; 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It worked fine on a small data set and was useful to confirm the deletion order and flush out some bad data and design oddities. Also it has no problem for few thousand to even tens of thousands of rows so it worked fine for the majority of the cases.&lt;br /&gt;
Where millions of rows were to be deleted it was taking long time and transaction log was growing rapidly. The full deletion script actually never ran to completion on production-grade data.&lt;/p&gt;
&lt;h2 id="second-iteration-batched-deletions"&gt;Second iteration: Batched deletions&lt;/h2&gt;
&lt;p&gt;For the large tables I tried batched deletes similar to:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;DECLARE &amp;#64;batch_size INT=5000;
delete_top:
  DELETE TOP(&amp;#64;batch_size) x FROM a_table x JOIN &amp;#64;TenantsToDelete t ON t.Id=x.TenantId;
  IF &amp;#64;batch_size = &amp;#64;&amp;#64;ROWCOUNT GOTO delete_top;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This version of the script was able to complete deletion of a single small-sized tenant for around 30 minutes, for a large one it took more than 15 hours - and I needed to remove around 40 of them. It was still no good.&lt;/p&gt;
&lt;h2 id="third-iteration-replace-delete-with-insert"&gt;Third iteration: Replace DELETE with INSERT&lt;/h2&gt;
&lt;p&gt;So &lt;code&gt;DELETE&lt;/code&gt; is slow and there is no much room for improvement. There is no &lt;em&gt;&amp;quot;bulk delete&amp;quot;&lt;/em&gt; kind of operation in MS SQL Server to boost the performance and I was not in a position to use &lt;code&gt;TRUNCATE&lt;/code&gt; or &lt;code&gt;DROP TABLE&lt;/code&gt; since I needed to preserve part of the data.&lt;/p&gt;
&lt;p&gt;While searching for solution I found this article about optimizing loading of data and the impact of minimally logged operations on I/O: &lt;a href="https://docs.microsoft.com/en-us/previous-versions/sql/sql-server-2008/dd425070(v=sql.100)?redirectedfrom=MSDN"&gt;The Data Loading Performance Guide&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In essence we can insert data fast (with minimal logging) when these conditions are met:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;database is in &lt;code&gt;SIMPLE&lt;/code&gt; or &lt;code&gt;BULK_LOGGED&lt;/code&gt; recovery mode,&lt;/li&gt;
&lt;li&gt;target table is a &lt;em&gt;heap table&lt;/em&gt; (without clustered index),&lt;/li&gt;
&lt;li&gt;a &lt;code&gt;WITH (TABLOCK)&lt;/code&gt; hint is used with the insert (allows exclusive lock on the target table).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is additional performance boost using the latter in SQL Server 2016 and higher - &lt;code&gt;INSERT...SELECT WITH (TABLOCK)&lt;/code&gt; may use parallel inserts.&lt;/p&gt;
&lt;p&gt;So back to my deletion problem - what if instead of delete I do the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a heap table with exactly the same structure as the one I want to delete from&lt;/li&gt;
&lt;li&gt;Move to the heap table data I want to &lt;strong&gt;keep&lt;/strong&gt; using &lt;code&gt;INSERT...SELECT WITH (TABLOCK)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Drop the original table (metadata-only operation, fast)&lt;/li&gt;
&lt;li&gt;Rename the heap table to original one's name&lt;/li&gt;
&lt;li&gt;Create indexes, constraints, references etc.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;TL;DR;&lt;/strong&gt;&lt;/em&gt; Yes, it worked and it was way faster without growing the transaction log since all operations were minimally logged.&lt;br /&gt;
Replacing deletion of my around 30 problematic tables with this technique lead to deletion of everything but a mid-sized tenant data to complete in 15 minutes.&lt;br /&gt;
Deletion of everything but the largest tenant (moving the most data) completed in 1 hour.&lt;/p&gt;
&lt;h3 id="generating-the-sql"&gt;Generating the SQL&lt;/h3&gt;
&lt;p&gt;Although I was quite happy with the performance, there was another problem - scripting the heap tables, moving the data and re-creating the indexes by hand is tedious and error prone (imagine doing it for 30 tables).&lt;/p&gt;
&lt;p&gt;SQL Server Management Studio has a lot of scripting capabilities and fortunately they are available via the &lt;a href="https://docs.microsoft.com/en-us/sql/relational-databases/server-management-objects-smo/sql-server-management-objects-smo-programming-guide?view=sql-server-ver15"&gt;SQL Management Objects&lt;/a&gt; (SMO). It is a set of .NET Framework assemblies meaning that they can be used from PowerShell also.&lt;/p&gt;
&lt;p&gt;Loading the assemblies and creating &lt;code&gt;Microsoft.SqlServer.Management.Smo.Server&lt;/code&gt; instance is the first step:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;[System.Reflection.Assembly]::LoadWithPartialName(&amp;quot;Microsoft.SqlServer.Smo&amp;quot;)
[System.Reflection.Assembly]::LoadWithPartialName(&amp;quot;Microsoft.SqlServer.ConnectionInfo&amp;quot;)

$connection = New-Object System.Data.SqlClient.SqlConnection $sqlConnectionString
$serverConnection = New-Object Microsoft.SqlServer.Management.Common.ServerConnection $connection

[Microsoft.SqlServer.Management.Smo.Server] $server = New-Object Microsoft.SqlServer.Management.Smo.Server $serverConnection
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To create a heap table:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;$scripter = New-Object Microsoft.SqlServer.Management.Smo.Scripter $server

$script = $scripter.Script(($server.Databases[$connection.Database].Tables[$table]))

# scripter can only generate scripts for existing objects so we need to change the table name 
$script[2] = $script[2].Replace(&amp;quot;CREATE TABLE [dbo].[$table]&amp;quot;, &amp;quot;CREATE TABLE [dbo].[$heapTable]&amp;quot;)
$script
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For moving the data, we can generate the columns list to use in the insert statement:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;$columnNames = $server.Databases[$connection.Database].Tables[$table].Columns | Select-Object $_ | ForEach-Object {$_.Name}
$columns = [System.String]::Join(', ', $columnNames)

$sql = New-Object System.Collections.Specialized.StringCollection
$sql.Add(&amp;quot;INSERT INTO $heapTable WITH(TABLOCK)&amp;quot;) &amp;gt; $null
$sql.Add(&amp;quot;($columns)&amp;quot;) &amp;gt; $null
$sql.Add(&amp;quot;SELECT&amp;quot;) &amp;gt; $null
$sql.Add(&amp;quot;$columns FROM $table &amp;quot;) &amp;gt; $null
$sql.Add(&amp;quot;$whereClause&amp;quot;) &amp;gt; $null

$sql
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are some pitfalls to consider:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;beware of &lt;code&gt;TIMESTAMP&lt;/code&gt; columns: you cannot insert them&lt;/li&gt;
&lt;li&gt;beware of &lt;code&gt;IDENTITY&lt;/code&gt; columns: I found it hard to change column to identity so create it as an identity and use &lt;code&gt;SET IDENTITY_INSERT ON/OFF&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;again for &lt;code&gt;IDENTITY&lt;/code&gt; columns: the scripter will capture the current identity seed when generating the script but it will be executed at some later point and the seed will be different. Thus in my script I capture the seed from the original table column after the data is moved and reseed the column of the heap table. To find the identity column you can use something like:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;$server.Databases[$connection.Database].Tables[$table].Columns | Where-Object { $_.Identity -eq $true } | Select-Object -First 1
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;depending on your &lt;code&gt;$whereClause&lt;/code&gt; you may need to prefix the columns in the select list, e.g. when using something like &lt;code&gt;SELECT x.ID FROM x_Table x JOIN y_Table y on y.Id=x.Id...&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before dropping the original table we need to remove all references to it's Primary Key:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;$inboundForeignKeys = $server.Databases[$connection.Database].Tables[$table].EnumForeignKeys()
foreach($foreignKey in $inboundForeignKeys)
{
    Write-Output &amp;quot;ALTER TABLE [dbo].[$($foreignKey['Table_Name'])] DROP CONSTRAINT [$($foreignKey['Name'])];&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you have triggers or schema-bound objects or anything preventing table to be drop should be taken care of - luckily in my case there was not.
Then we can drop the original table and rename the heap table. Nothing special required - &lt;code&gt;DROP TABLE&lt;/code&gt; and &lt;code&gt;sp_rename&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The last step is to create primary and foreign keys, defaults, constraints and indexes:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;# indexes, including primary key
$scripter = New-Object Microsoft.SqlServer.Management.Smo.Scripter $server
$scripter.Options.NoCollation = $true
$scripter.Options.DriPrimaryKey = $true
$scripter.Options.DriUniqueKeys = $true
$scripter.Options.ClusteredIndexes = $true
$scripter.Options.NonClusteredIndexes = $true
$scripter.Options.Indexes = $true
$scripter.Options.DriAll = $true

[Microsoft.SqlServer.Management.Smo.SqlSmoObject[]]$indexes = $server.Databases$connection.Database].Tables[$table].Indexes

$scripter.Script($indexes)

# default constraints
$columns = $server.Databases[$connection.Database].Tables[$table].Columns
foreach($column in $columns)
{
    if($column.DefaultConstraint)
    {
        $column.DefaultConstraint.Script()
    }
}

# foreign keys
[Microsoft.SqlServer.Management.Smo.SqlSmoObject[]]$foreignKeys = $server.Databases[$connection.Database].Tables[$table].ForeignKeys

$scripter.Script($foreignKeys)

# foreign keys referencing the table
$inboundForeignKeys = $server.Databases[$connection.Database].Tables[$table].EnumForeignKeys()
foreach($foreignKey in $inboundForeignKeys)
{
    [Microsoft.SqlServer.Management.Smo.SqlSmoObject[]]$key = $server.Databases[$connection.Database].Tables[$foreignKey['Table_Name']].ForeignKeys[$foreignKey['Name']]

    $scripter.Script($key)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are the most important the pieces I needed to generate the SQL.&lt;/p&gt;
&lt;p&gt;In practice I turned the initial deletion SQL script into a kind of template containing the standard deletion statements and occasionally a palceholder with the table name and where clause defining the data to be kept. Then a PowerShell script will read the template and replace the placeholders with SQL statements generated using the steps described above.&lt;/p&gt;
&lt;p&gt;Having automated SQL script generation based on the actual metadata has two main advantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;removes the repetitive (and boring) manual work&lt;/li&gt;
&lt;li&gt;when the schema changes (e.g. a table is added or altered, column is changed or dropped, index is created, etc.) the SQL script can be easily generated again&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Replacing a regular SQL deletes with inserting in heap table and dropping the original one has achieved a satisfying performance boost. It is a combination of SQL Server's data loading capabilities and minimally logged operations. In my case it worked good since I needed to keep (move) 20-30% of the table's data.&lt;/p&gt;
&lt;p&gt;Implementation is more complex than regular deletes so an automated SQL script generation comes handy.&lt;/p&gt;
&lt;p&gt;The downside of the solution is it's complexity so I would recommend it only as a last resort.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;Deleting large amount of rows (like millions of them) from a table has a downside of being slow and making a transaction log file to explode in terms of size. Here is an approach that worked for me overcome these obstacles.&lt;/p&gt;</summary>
	</entry>
</feed>